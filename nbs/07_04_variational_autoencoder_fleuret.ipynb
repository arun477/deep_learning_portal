{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a03916",
   "metadata": {},
   "source": [
    "https://fleuret.org/dlc/materials/dlc-slides-7-4-VAE.pdf (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import default_collate, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f46a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mnist (/root/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd25dec8aaea4639bd4ae8eab0078d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform(b):\n",
    "    b['image'] = [TF.to_tensor(o) for o in b['image']]\n",
    "    return b\n",
    "\n",
    "def collate_dict(b):\n",
    "    c = default_collate(b)\n",
    "    return (c['image'], c['label'])\n",
    "\n",
    "bs = 1024\n",
    "\n",
    "data = load_dataset('mnist')\n",
    "train_data, valid_data = data['train'].with_transform(transform), data['test'].with_transform(transform)\n",
    "train_dl = DataLoader(train_data, batch_size=bs, shuffle=True, collate_fn=collate_dict) \n",
    "valid_dl = DataLoader(valid_data, batch_size=bs*2, shuffle=False, collate_fn=collate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 28, 28]) torch.Size([1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "print(xb.shape, yb.shape)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(xb[0].permute(1,2,0));\n",
    "plt.axis('off');yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90784545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, nb_channels, latent_dim):\n",
    "        super().__init__() \n",
    "    \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, nb_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=5),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=5),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(nb_channels, nb_channels, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(nb_channels, latent_dim*2, kernel_size=4),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, nb_channels, kernel_size=4),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(nb_channels, nb_channels, kernel_size=5),\n",
    "            nn.BatchNorm2d(nb_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(nb_channels, 1, kernel_size=5)\n",
    "        )  \n",
    "    \n",
    "    def encode(self, x):\n",
    "        out = self.encoder(x).view(x.shape[0], 2, -1)\n",
    "        mu,log_var = out[:,0],out[:,1]\n",
    "        return mu,log_var\n",
    "    \n",
    "    def sample_gaussian(self, mu, log_var):\n",
    "        std = log_var.mul(0.5).exp()\n",
    "        return mu + torch.randn(mu.size(), device=mu.device) * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        mu_hat = self.decoder(z.view(z.shape[0], -1, 1, 1))\n",
    "        return mu_hat,mu_hat.new_zeros(mu_hat.size())\n",
    "    \n",
    "    def log_p_gaussian(self, x, mu_hat, log_var_hat):\n",
    "        mu_hat,log_var_hat,x = mu_hat.flatten(1),log_var_hat.flatten(1),x.flatten(1)\n",
    "        var_hat = log_var_hat.exp()\n",
    "        return -0.5*(((x-mu_hat).pow(2)/var_hat) + log_var_hat + math.log(2*math.pi)).sum(1)     \n",
    "    \n",
    "    def dkl(self, dist_a, dist_b):\n",
    "        mu_a,log_var_a = dist_a[0].flatten(1),dist_a[1].flatten(1)\n",
    "        mu_b,log_var_b = dist_b[0].flatten(1),dist_b[1].flatten(1)\n",
    "        var_a = log_var_a.exp()\n",
    "        var_b = log_var_b.exp()\n",
    "        return  0.5 * (log_var_b - log_var_a - 1 + (mu_a - mu_b).pow(2) / var_b + var_a / var_b).sum(1)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        mu,log_var = self.encode(x)\n",
    "        \n",
    "        z = self.sample_gaussian(mu, log_var)\n",
    "        \n",
    "        mu_hat,log_var_hat = self.decode(z)\n",
    "        \n",
    "        log_p_given_z = self.log_p_gaussian(x, mu_hat, log_var_hat)\n",
    "        \n",
    "        z_prior = (torch.zeros_like(mu), torch.randn_like(log_var))\n",
    "        dkl_q_vs_z = self.dkl((mu, log_var), z_prior)\n",
    "        \n",
    "        loss = -(log_p_given_z-dkl_q_vs_z).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797a08a-f64c-486a-9209-8755fc120b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1139.5625, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = VAE(nb_channels=8, latent_dim=32).to(device)\n",
    "o = m(xb.to(device))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194993eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:793.0808\n",
      "loss:758.6329\n",
      "loss:757.0048\n",
      "loss:765.5275\n",
      "loss:765.4188\n",
      "loss:766.4263\n",
      "loss:766.4310\n",
      "loss:765.2722\n",
      "loss:758.7982\n",
      "loss:754.8967\n"
     ]
    }
   ],
   "source": [
    "model = VAE(nb_channels=8, latent_dim=32).to(device)\n",
    "lr = 5e-1\n",
    "epochs = 10\n",
    "opt = torch.optim.AdamW(model.parameters(),lr)\n",
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        xb = xb.to(device)\n",
    "        loss = model(xb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(f'loss:{loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac3b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
