{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2af0f4",
   "metadata": {},
   "source": [
    "https://github.com/AnswerDotAI/fsdp_qlora/blob/main/train.py (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install hqq\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2152b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'bitsandbytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e4849808c965>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitsandbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear4bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParams4bit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "import types\n",
    "\n",
    "try:\n",
    "    from hqq.core.quantize import HQQLinear, HQQBackend, BaseQuantizeConfig\n",
    "except ImportError:\n",
    "    HQQLinear = None\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, args, log_to='stdout', project_name='fsdp_qlora',\n",
    "                 entity=None, group=None, name=None, rank=0):\n",
    "        self.log_to = log_to\n",
    "        if self.log_to == 'wandb' and rank==0:\n",
    "            import wandb\n",
    "            wandb.init(\n",
    "                project=project_name,\n",
    "                entity=entity,\n",
    "                group=group,\n",
    "                name=name,\n",
    "                cofig=args\n",
    "            )\n",
    "\n",
    "    def log(self, d:Dict, rank:int):\n",
    "        if rank != 0:\n",
    "            return\n",
    "        if self.log_to == \"tqdm\":\n",
    "            for k,v in d.items():\n",
    "                tqdm.write(f'{k}: {v}')\n",
    "        elif self.log_to == 'wandb':\n",
    "            wandb.log(d)\n",
    "        elif self.log_to == 'stdout':\n",
    "            for k,v in d.item():\n",
    "                print(f'{k}: {v}')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress_bar(progress_bar:tqdm, epoch:int, log_loss:float, log_lr:float, rank:int):\n",
    "    if rank==0:\n",
    "        if log_lr >= 0:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}, lr {log_lr:.2e}\", refresh=True)\n",
    "        else:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}\", refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:Union[dict,None]=None,\n",
    "                  skip_modules:List[str]=['lm_head'], **kwargs):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, nn.Linear) and name not in skip_modules:\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            elif issubclass(linear_replacement, HQQLinear):\n",
    "                model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f78096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(\n",
    "    nn.Linear(5, 10),\n",
    "    nn.Sequential(nn.Linear(10, 50), nn.ReLU())\n",
    ")\n",
    "m = m.to('cuda')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.float32 True cuda:0\n",
      "torch.Size([10]) torch.float32 True cuda:0\n",
      "torch.Size([50, 10]) torch.float32 True cuda:0\n",
      "torch.Size([50]) torch.float32 True cuda:0\n"
     ]
    }
   ],
   "source": [
    "for p in m.parameters():\n",
    "    print(p.shape, p.dtype, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ff371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear4bit(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear4bit(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = replace_linear(m, linear_replacement=Linear4bit).to('cuda')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfe736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1]) torch.uint8 False cuda:0\n",
      "torch.Size([10]) torch.float32 True cuda:0\n",
      "torch.Size([250, 1]) torch.uint8 False cuda:0\n",
      "torch.Size([50]) torch.float32 True cuda:0\n"
     ]
    }
   ],
   "source": [
    "for p in r.parameters():\n",
    "    print(p.shape, p.dtype, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4BYywc_oAEk5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantized_meta_for_peft(model:nn.Module):\n",
    "  def temp_to_method(self, *args, **kwargs):\n",
    "    return self\n",
    "  for param in model.parameters():\n",
    "    if isinstance(param, Params4bit):\n",
    "      param.quant_state._orig_to = param.quant_state.to\n",
    "      param.quant_state.to = types.MethodType(temp_to_method, param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJUj0tpGCKXf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantized_peft_meta_for_training(model:nn.Module):\n",
    "  for param in model.parameters():\n",
    "    if isinstance(param, Params4bit) and hasattr(param.quant_state, '_orig_to'):\n",
    "      param.quant_state.to = param.quant_state._orig_to\n",
    "      param.quant_state._orig_to = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r8XKqV6FEwt5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_key: nn\n",
      "value_key: Conv2d\n",
      "_ .\n"
     ]
    }
   ],
   "source": [
    "name = \"nn.Conv2d\"\n",
    "module_key, _, value_key = name.rpartition('.')\n",
    "\n",
    "print(\"module_key:\", module_key)\n",
    "print(\"value_key:\", value_key)\n",
    "print('_', _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZJO3ykWC2-O",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False, quant_method:str='bnb'):\n",
    "\n",
    "  def place_on_device(value):\n",
    "    if is_meta_rank:\n",
    "      device = 'meta'\n",
    "    elif low_memory:\n",
    "      device = 'cpu'\n",
    "    return value.to(device=device)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "      if verbose:\n",
    "        print(f'skipping {name} because it is in skip_names')\n",
    "      return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "      submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "      print(f'module {module_key} not found:\\n{e}')\n",
    "      return\n",
    "\n",
    "    try:\n",
    "      if quant_method=='bnb':\n",
    "          param = submodule.get_parameter(value_key)\n",
    "          if isinstance(param, Params4bit):\n",
    "            value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "            if is_meta_rank:\n",
    "              value = type(param)(value.data.to('meta'), **value.__dict__)\n",
    "            elif low_memory:\n",
    "              value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "          else:\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "      elif quant_method=='hqq':\n",
    "        if isinstance(submodule, HQQLinear):\n",
    "          if value_key == 'weight':\n",
    "            submodule.linear_layer.to_empty(device=device)\n",
    "            submodule.linear_layer.weight.data.copy_(value.to(device=device, dtype=dtype))\n",
    "            submodule.initialize()\n",
    "\n",
    "            if is_meta_rank:\n",
    "              setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to('meta')))\n",
    "            elif low_memory:\n",
    "              setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"cpu\")))\n",
    "            submodule.in_gpu = False\n",
    "\n",
    "          if value_key == \"bias\":\n",
    "            raise ValueError('bias not supported in HQQLinear yet')\n",
    "        else:\n",
    "          param = submodule.get_parameter(value_key)\n",
    "          value = type(param)(place_on_device(value).data)\n",
    "\n",
    "    except AttributeError:\n",
    "      # remove pass from source code\n",
    "      value = place_on_device(value)\n",
    "\n",
    "    if HQQLinear is None or not isinstance(submodule, HQQLinear):\n",
    "      setattr(submodule, value_key, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19TSiUWJaL22",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### instruction:\\n{instruction}\\n\\n### input:\\n{input}\\n\\n### response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"below is an instruction that describes a task. \"\n",
    "        \"write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### instrucion:\\n{instruction}\\n\\n### response:\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4-ZKcN9Ia2x5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GS8kCcOKbmEz",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, style=\"alpaca\"):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.style = style\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        IGNORE_INDEX = -100  # The default setting in CrossEntropyLoss\n",
    "        if self.style == \"guanaco\":\n",
    "            prompt = self.dataset[index][\"text\"].split(\"### Assistant: \")[0]\n",
    "            example = self.dataset[index][\"text\"]\n",
    "        elif self.style == \"qna\":\n",
    "            prompt_template = \"###Context:\\n{context}\\n###Question:\\n{question}\\n###Answer:\\n\"\n",
    "            sample = self.dataset[index]\n",
    "            prompt = prompt_template.format_map(sample)\n",
    "            example = prompt + sample['answer']\n",
    "        else: # Alpaca\n",
    "            ann = self.dataset[index]\n",
    "            if ann.get(\"input\", \"\") == \"\":\n",
    "                prompt = PROMPT_DICT[\"prompt_no_input\"].format_map(ann)\n",
    "            else:\n",
    "                prompt = PROMPT_DICT[\"prompt_input\"].format_map(ann)\n",
    "            example = prompt + ann[\"output\"]\n",
    "\n",
    "        prompt = torch.tensor(\n",
    "            self.tokenizer.encode(prompt), dtype=torch.int64\n",
    "        )\n",
    "        example = self.tokenizer.encode(example)\n",
    "        example.append(self.tokenizer.eos_token_id)\n",
    "        example = torch.tensor(\n",
    "            example, dtype=torch.int64\n",
    "        )\n",
    "        labels = copy.deepcopy(example)\n",
    "        labels[: len(prompt)] = -1\n",
    "        example_mask = example.ge(0)\n",
    "        label_mask = labels.ge(0)\n",
    "        example[~example_mask] = 0\n",
    "        labels[~label_mask] = IGNORE_INDEX\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": example.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "            \"attention_mask\":example_mask.tolist(),\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
