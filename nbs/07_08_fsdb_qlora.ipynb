{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2af0f4",
   "metadata": {},
   "source": [
    "https://github.com/AnswerDotAI/fsdp_qlora/blob/main/train.py (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install hqq\n",
    "# !pip install wandb\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2152b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "import types\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "import copy\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "try:\n",
    "    from hqq.core.quantize import HQQLinear, HQQBackend, BaseQuantizeConfig\n",
    "except ImportError:\n",
    "    HQQLinear = None\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, args, log_to='stdout', project_name='fsdp_qlora',\n",
    "                 entity=None, group=None, name=None, rank=0):\n",
    "        self.log_to = log_to\n",
    "        if self.log_to == 'wandb' and rank==0:\n",
    "            import wandb\n",
    "            wandb.init(\n",
    "                project=project_name,\n",
    "                entity=entity,\n",
    "                group=group,\n",
    "                name=name,\n",
    "                cofig=args\n",
    "            )\n",
    "\n",
    "    def log(self, d:Dict, rank:int):\n",
    "        if rank != 0:\n",
    "            return\n",
    "        if self.log_to == \"tqdm\":\n",
    "            for k,v in d.items():\n",
    "                tqdm.write(f'{k}: {v}')\n",
    "        elif self.log_to == 'wandb':\n",
    "            wandb.log(d)\n",
    "        elif self.log_to == 'stdout':\n",
    "            for k,v in d.item():\n",
    "                print(f'{k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress_bar(progress_bar:tqdm, epoch:int, log_loss:float, log_lr:float, rank:int):\n",
    "    if rank==0:\n",
    "        if log_lr >= 0:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}, lr {log_lr:.2e}\", refresh=True)\n",
    "        else:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}\", refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:Union[dict,None]=None,\n",
    "                  skip_modules:List[str]=['lm_head'], **kwargs):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, nn.Linear) and name not in skip_modules:\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            elif issubclass(linear_replacement, HQQLinear):\n",
    "                model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f78096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(\n",
    "    nn.Linear(5, 10),\n",
    "    nn.Sequential(nn.Linear(10, 50), nn.ReLU())\n",
    ")\n",
    "# m = m.to('cuda')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.float32 True cpu\n",
      "torch.Size([10]) torch.float32 True cpu\n",
      "torch.Size([50, 10]) torch.float32 True cpu\n",
      "torch.Size([50]) torch.float32 True cpu\n"
     ]
    }
   ],
   "source": [
    "for p in m.parameters():\n",
    "    print(p.shape, p.dtype, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ff371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear4bit(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear4bit(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = replace_linear(m, linear_replacement=Linear4bit)\n",
    "# r = r.to('cuda')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfe736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.float32 False cpu\n",
      "torch.Size([10]) torch.float32 True cpu\n",
      "torch.Size([50, 10]) torch.float32 False cpu\n",
      "torch.Size([50]) torch.float32 True cpu\n"
     ]
    }
   ],
   "source": [
    "for p in r.parameters():\n",
    "    print(p.shape, p.dtype, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4BYywc_oAEk5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantized_meta_for_peft(model:nn.Module):\n",
    "  def temp_to_method(self, *args, **kwargs):\n",
    "    return self\n",
    "  for param in model.parameters():\n",
    "    if isinstance(param, Params4bit):\n",
    "      param.quant_state._orig_to = param.quant_state.to\n",
    "      param.quant_state.to = types.MethodType(temp_to_method, param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJUj0tpGCKXf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantized_peft_meta_for_training(model:nn.Module):\n",
    "  for param in model.parameters():\n",
    "    if isinstance(param, Params4bit) and hasattr(param.quant_state, '_orig_to'):\n",
    "      param.quant_state.to = param.quant_state._orig_to\n",
    "      param.quant_state._orig_to = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r8XKqV6FEwt5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_key: nn\n",
      "value_key: Conv2d\n",
      "_ .\n"
     ]
    }
   ],
   "source": [
    "name = \"nn.Conv2d\"\n",
    "module_key, _, value_key = name.rpartition('.')\n",
    "\n",
    "print(\"module_key:\", module_key)\n",
    "print(\"value_key:\", value_key)\n",
    "print('_', _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZJO3ykWC2-O",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False, quant_method:str='bnb'):\n",
    "\n",
    "  def place_on_device(value):\n",
    "    if is_meta_rank:\n",
    "      device = 'meta'\n",
    "    elif low_memory:\n",
    "      device = 'cpu'\n",
    "    return value.to(device=device)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "      if verbose:\n",
    "        print(f'skipping {name} because it is in skip_names')\n",
    "      return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "      submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "      print(f'module {module_key} not found:\\n{e}')\n",
    "      return\n",
    "\n",
    "    try:\n",
    "      if quant_method=='bnb':\n",
    "          param = submodule.get_parameter(value_key)\n",
    "          if isinstance(param, Params4bit):\n",
    "            value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "            if is_meta_rank:\n",
    "              value = type(param)(value.data.to('meta'), **value.__dict__)\n",
    "            elif low_memory:\n",
    "              value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "          else:\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "      elif quant_method=='hqq':\n",
    "        if isinstance(submodule, HQQLinear):\n",
    "          if value_key == 'weight':\n",
    "            submodule.linear_layer.to_empty(device=device)\n",
    "            submodule.linear_layer.weight.data.copy_(value.to(device=device, dtype=dtype))\n",
    "            submodule.initialize()\n",
    "\n",
    "            if is_meta_rank:\n",
    "              setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to('meta')))\n",
    "            elif low_memory:\n",
    "              setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"cpu\")))\n",
    "            submodule.in_gpu = False\n",
    "\n",
    "          if value_key == \"bias\":\n",
    "            raise ValueError('bias not supported in HQQLinear yet')\n",
    "        else:\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "\n",
    "    except AttributeError:\n",
    "      # remove pass from source code\n",
    "      value = place_on_device(value)\n",
    "\n",
    "    if HQQLinear is None or not isinstance(submodule, HQQLinear):\n",
    "      setattr(submodule, value_key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19TSiUWJaL22",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### instruction:\\n{instruction}\\n\\n### input:\\n{input}\\n\\n### response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"below is an instruction that describes a task. \"\n",
    "        \"write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### instrucion:\\n{instruction}\\n\\n### response:\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4-ZKcN9Ia2x5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "  def __init__(self, dataset, tokenizer, style='alpaca'):\n",
    "    self.dataset = dataset\n",
    "    self.tokenizer = tokenizer\n",
    "    self.style = style\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    IGNORE_INDEX = -100\n",
    "    if self.style == 'guanaco':\n",
    "      prompt = self.dataset[index]['text'].split('### Assistant: ')[0]\n",
    "      example = self.dataset[index]['text']\n",
    "    elif self.style == 'qna':\n",
    "      prompt_template = \"###Context:\\n{context}\\n###Question:\\n{question}\\n###Answer:\\n\"\n",
    "      sample = self.dataset[index]\n",
    "      prompt = prompt_template.format_map(sample)\n",
    "      example = prompt + sample['answer']\n",
    "    else: #alpaca\n",
    "      ann = self.dataset[index]\n",
    "      if ann.get(\"input\", \"\") == \"\":\n",
    "        prompt = PROMPT_DICT['prompt_no_input'].format_map(ann)\n",
    "      else:\n",
    "        prompt = PROMPT_DICT['prompt_input'].format_map(ann)\n",
    "      example = prompt + ann['output']\n",
    "\n",
    "    prompt = torch.tensor(\n",
    "        self.tokenizer.encode(prompt), dtype=torch.int64\n",
    "    )\n",
    "    example = self.tokenizer.encode(example)\n",
    "    example.append(self.tokenizer.eos_token_id)\n",
    "    example = torch.tensor(\n",
    "        example, dtype=torch.int64\n",
    "    )\n",
    "    labels = copy.deepcopy(example)\n",
    "    labels[: len(prompt)] = -1\n",
    "    example_mask = example.ge(0)\n",
    "    label_mask = labels.ge(0)\n",
    "    example[~example_mask] = 0\n",
    "    labels[~label_mask] = IGNORE_INDEX\n",
    "\n",
    "    return  {\n",
    "        'input_ids': example.tolist(),\n",
    "        'labels': labels.tolist(),\n",
    "        'attention_mask': example_mask.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L2y-PWtjfzSk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(tokenizer:PreTrainedTokenizerFast, args:Dict):\n",
    "  from datasets import Dataset, load_dataset\n",
    "\n",
    "  if args['dataset'] == 'alpaca':\n",
    "    dataset = load_dataset('yahma/alpaca-cleaned')['train']\n",
    "  elif args['dataset'] == 'alpaca_sample':\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split='train[:512]')\n",
    "  elif args['dataset'] == 'dummy':\n",
    "    dataset = Dataset.from_dict({\n",
    "        'instruction': [\"instruction\"]*512,\n",
    "        'input': [\"input\"]*512,\n",
    "        'output': ['output'*10000]*512\n",
    "    })\n",
    "  elif args['dataset'] == 'guanaco':\n",
    "    dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split='train')\n",
    "  elif args['dataset'] == 'sql':\n",
    "    dataset = load_dataset(\"knowrohit07/know_sql\")['validation']\n",
    "    dataset = dataset.shuffle(seed=args['seed'])\n",
    "    dataset = dataset.select(range(1000,len(dataset)))\n",
    "\n",
    "  dataset = dataset.select(range(0, len(dataset)-len(dataset)%(args['batch_size']*args['gradient_accumulation_steps'])))\n",
    "\n",
    "  if args['dataset'] == 'guanaco':\n",
    "    dataset = InstructionDataset(dataset, tokenizer, style='guanaco')\n",
    "  elif args['dataset'] == 'sql':\n",
    "    dataset = InstructionDataset(dataset, tokenizer, style='qna')\n",
    "  else: #alpaca\n",
    "    dataset = InstructionDataset(dataset, tokenizer, style='alpaca')\n",
    "\n",
    "  def collate_fn(batch, with_attention_mask=False):\n",
    "    input_ids  = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_masks = [torch.tensor(item['attension_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)[:,:args['context_length']]\n",
    "    if with_attention_mask:\n",
    "      attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)[:,:args['context_length']]\n",
    "    else:\n",
    "      attention_masks = None\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)[:,:args['context_length']]\n",
    "    return  {\n",
    "        'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels\n",
    "    }\n",
    "\n",
    "  sampler = DistributedSampler(dataset, seed=args['seed'])\n",
    "\n",
    "  dataloader = DataLoader(dataset, batch_size=args['batch_size'], collate_fn=collate_fn, sampler=sampler)\n",
    "\n",
    "  return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vKKX8rrSf9H4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def _get_cosine_one_cycle_lr_lambda(\n",
    "    current_step:int, *, num_warmup_steps:int, num_training_steps:int, min_lr_fraction=0.1\n",
    ") :\n",
    "    if current_step < num_warmup_steps:\n",
    "        return  float(current_step) / float(max(1, num_warmup_steps))\n",
    "    scale_term = (1-min_lr_fraction)\n",
    "    progress = float(current_step-num_warmup_steps) / float(max(1, num_training_steps-num_warmup_steps))\n",
    "    return (math.cos(math.pi*progress)+1) * 0.5 * scale_term + min_lr_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa58d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
