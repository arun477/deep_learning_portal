{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3708fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ee237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "from bitsandbytes.nn import Linear4bit\n",
    "\n",
    "try:\n",
    "    from hqq.core.quantize import HQQLinear, HQQBackend, BaseQuantizeConfig\n",
    "except ImportError:\n",
    "    HQQLinear = None\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cdd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, args, log_to='stdout', project_name='fsdp_qlora',\n",
    "                 entity=None, group=None, name=None, rank=0):\n",
    "        self.log_to = log_to\n",
    "        if self.log_to == 'wandb' and rank==0:\n",
    "            import wandb\n",
    "            wandb.init(\n",
    "                project=project_name,\n",
    "                entity=entity,\n",
    "                group=group,\n",
    "                name=name,\n",
    "                cofig=args\n",
    "            )\n",
    "    \n",
    "    def log(self, d:Dict, rank:int):\n",
    "        if rank != 0:\n",
    "            return \n",
    "        if self.log_to == \"tqdm\":\n",
    "            for k,v in d.items():\n",
    "                tqdm.write(f'{k}: {v}')\n",
    "        elif self.log_to == 'wandb':\n",
    "            wandb.log(d)\n",
    "        elif self.log_to == 'stdout':\n",
    "            for k,v in d.item():\n",
    "                print(f'{k}: {v}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress_bar(progress_bar:tqdm, epoch:int, log_loss:float, log_lr:float, rank:int):\n",
    "    if rank==0:\n",
    "        if log_lr >= 0:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}, lr {log_lr:.2e}\", refresh=True)\n",
    "        else:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}\", refresh=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de608a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:dict|None=None,\n",
    "                   skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        linear_replacement (`torch.nn.Module`):\n",
    "            The linear module that replaces the old one. Only expects standard arguments.\n",
    "            If other arguments need to be passed, use a lambda.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear) and name not in skip_modules:\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            elif issubclass(linear_replacement, HQQLinear):\n",
    "                model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44107d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:Union[dict,None]=None,\n",
    "                  skip_modules:List[str]=['lm_head'], **kwargs):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "            \n",
    "        \n",
    "        if isinstance(module, nn.Linear) and name not in skip_modules:\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                module._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            elif issubclass(linear_replacement, HQQLinear):\n",
    "                model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc89324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(\n",
    "    nn.Linear(5, 10),\n",
    "    nn.Sequential(nn.Linear(10, 50), nn.ReLU())\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f7c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "OrderedDict()\n",
      "1\n",
      "Linear(in_features=10, out_features=50, bias=True)\n",
      "ReLU()\n",
      "OrderedDict([('0', Linear(in_features=10, out_features=50, bias=True)), ('1', ReLU())])\n"
     ]
    }
   ],
   "source": [
    "for name, module in m.named_children():\n",
    "    print(name)\n",
    "    for l in module.children():\n",
    "        print(l)\n",
    "    print(module._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c01a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
