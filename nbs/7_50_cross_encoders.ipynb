{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9363798b-4c4e-4305-ac75-72de3f8fb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc848b9b-c6c4-4f27-8361-d3c26caf01af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.1523685, -6.2870407], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\n",
    "scores = model.predict(\n",
    "    [('How many people live in Berlin?', \n",
    "      'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n",
    "    ('How many people live in Berlin?', 'Berlin is well known for its museums.')]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d45f016-d289-4425-a6b9-c739dd295405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46552283, 0.6350212 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CrossEncoder('cross-encoder/stsb-TinyBERT-L-4')\n",
    "scores = model.predict(\n",
    "    [(\"The weather today is beautiful\", \"It's raining!\"), \n",
    "                        (\"The weather today is beautiful\", \"Today is a sunny day\")]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b198edc-aa6f-4ed0-9ec8-2336a32933b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('jamescalam/ai-arxiv-chunked')\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69464d6-2724-4369-b7ce-a7fc3399cec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1910.01108',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       " 'id': '1910.01108',\n",
       " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
       " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
       " 'authors': ['Victor Sanh',\n",
       "  'Lysandre Debut',\n",
       "  'Julien Chaumond',\n",
       "  'Thomas Wolf'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20191002',\n",
       " 'updated': '20200301',\n",
       " 'references': [{'id': '1910.01108'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e6ba8b-c7f7-4b39-a71a-d020647b0df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41584"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = dataset['train']['chunk']\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f18c3b6-5e62-4048-a431-53f09bad323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8de0fe0795c4e118ffb025d5d38bbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 512\n",
    "corpus_embeddings = bi_encoder.encode(chunks, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e4570e-bb86-45fc-82f8-ae3e82f4a56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 14679, 'score': 0.6051740646362305},\n",
       " {'corpus_id': 14725, 'score': 0.5746961832046509},\n",
       " {'corpus_id': 39564, 'score': 0.5672644376754761},\n",
       " {'corpus_id': 17387, 'score': 0.5642737150192261},\n",
       " {'corpus_id': 5628, 'score': 0.5358618497848511},\n",
       " {'corpus_id': 14805, 'score': 0.5182210206985474},\n",
       " {'corpus_id': 14802, 'score': 0.5044544339179993},\n",
       " {'corpus_id': 20652, 'score': 0.5011643767356873},\n",
       " {'corpus_id': 9761, 'score': 0.499434232711792},\n",
       " {'corpus_id': 9755, 'score': 0.4980108439922333},\n",
       " {'corpus_id': 9763, 'score': 0.49280524253845215},\n",
       " {'corpus_id': 20653, 'score': 0.48917996883392334},\n",
       " {'corpus_id': 20711, 'score': 0.4854634702205658},\n",
       " {'corpus_id': 14750, 'score': 0.4836026430130005},\n",
       " {'corpus_id': 14680, 'score': 0.4797966778278351},\n",
       " {'corpus_id': 14716, 'score': 0.4774951934814453},\n",
       " {'corpus_id': 14806, 'score': 0.47698211669921875},\n",
       " {'corpus_id': 35250, 'score': 0.46669578552246094},\n",
       " {'corpus_id': 14821, 'score': 0.46372905373573303},\n",
       " {'corpus_id': 20632, 'score': 0.4619232714176178},\n",
       " {'corpus_id': 14726, 'score': 0.46023669838905334},\n",
       " {'corpus_id': 14728, 'score': 0.4577748775482178},\n",
       " {'corpus_id': 14797, 'score': 0.45471128821372986},\n",
       " {'corpus_id': 9771, 'score': 0.45408281683921814},\n",
       " {'corpus_id': 20638, 'score': 0.4512876570224762}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "query = \"what is rlhf?\"\n",
    "top_k = 25\n",
    "query_emb = bi_encoder.encode(query, convert_to_tensor=True).cuda()\n",
    "hits = util.semantic_search(query_emb, corpus_embeddings, top_k=top_k)[0]\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "966cdd0a-d221-4283-bfd0-76cb5cb3e1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_corpus_ids = [hit['corpus_id'] for hit in hits]\n",
    "len(retrival_corpus_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0864605e-5fff-473c-879b-bc0d2ebc861c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.6051740646362305, source:http://arxiv.org/pdf/2204.05862\n",
      "learning from human feedback, which we improve on a roughly weekly cadence. See Section 2.3.\n",
      "4This means that our helpfulness dataset goes ‘up’ in desirability during the conversation, while our harmlessness\n",
      "dataset goes ‘down’ in desirability. We chose the latter to thoroughly explore bad behavior, but it is likely not ideal\n",
      "for teaching good behavior. We believe this difference in our data distributions creates subtle problems for RLHF, and\n",
      "suggest that others who want to use RLHF to train safer models consider the analysis in Section 4.4.\n",
      "5\n",
      "1071081091010\n",
      "Number of Parameters0.20.30.40.50.6Mean Eval Acc\n",
      "Mean Zero-Shot Accuracy\n",
      "Plain Language Model\n",
      "RLHF\n",
      "1071081091010\n",
      "Number of Parameters0.20.30.40.50.60.7Mean Eval Acc\n",
      "Mean Few-Shot Accuracy\n",
      "Plain Language Model\n",
      "RLHFFigure 3 RLHF model performance on zero-shot and few-shot NLP tasks. For each model size, we plot\n",
      "the mean accuracy on MMMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, and\n",
      "TriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small\n",
      "-------------------\n",
      "\n",
      "score:0.5746961832046509, source:http://arxiv.org/pdf/2204.05862\n",
      "model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
      "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
      "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
      "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
      "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
      "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
      "region where RL reward remains linear inp\n",
      "KL.\n",
      "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
      "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
      "KLwith a geodesic length in the Fisher geometry.\n",
      "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
      "might hope to detect unexpected behaviors emerging during RL training.\n",
      "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
      "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
      "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
      "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
      "-------------------\n",
      "\n",
      "score:0.5672644376754761, source:http://arxiv.org/pdf/2307.09288\n",
      "31\n",
      "5 Discussion\n",
      "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
      "limitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
      "models (Section 5.3).\n",
      "5.1 Learnings and Observations\n",
      "Our tuning process revealed several interesting results, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ’s abilities to temporally\n",
      "organize its knowledge, or to call APIs for external tools.\n",
      "SFT (Mix)\n",
      "SFT (Annotation)\n",
      "RLHF (V1)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "Reward Model ScoreRLHF (V2)\n",
      "Figure 20: Distribution shift for progressive versions of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , from SFT models towards RLHF.\n",
      "Beyond Human Supervision. At the outset of the project, many among us expressed a preference for\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, hit in enumerate(hits[:3]):\n",
    "    sample = dataset['train'][hit['corpus_id']]\n",
    "    score = hit['score']\n",
    "    print(f\"score:{score}, source:{sample['source']}\")\n",
    "    print(f\"{sample['chunk']}\")\n",
    "    print('-------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b075a256-3297-4941-8fb9-3855f9bc017d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2227583 ,  2.2057667 ,  1.2897233 ,  5.048051  ,  4.4136815 ,\n",
       "        0.72113264,  1.2272785 ,  0.16303732,  2.563827  ,  2.9497519 ,\n",
       "        2.3555288 ,  1.3877895 ,  1.3640015 ,  1.584944  ,  2.229299  ,\n",
       "        0.8184781 ,  1.676284  ,  2.0563674 ,  1.5945492 ,  2.3106794 ,\n",
       "        0.9949666 ,  1.0842168 , -0.4363297 ,  2.2930846 ,  5.590805  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "cross_inp = [[query, chunks[hit['corpus_id']]] for hit in hits]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "cross_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d00084b-60e9-4a1a-be10-f1e9a8b52e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 14797, 'score': 0.45471128821372986, 'cross-score': 5.590805},\n",
       " {'corpus_id': 9755, 'score': 0.4980108439922333, 'cross-score': 5.048051},\n",
       " {'corpus_id': 9761, 'score': 0.499434232711792, 'cross-score': 4.4136815},\n",
       " {'corpus_id': 14725, 'score': 0.5746961832046509, 'cross-score': 2.9497519},\n",
       " {'corpus_id': 14680, 'score': 0.4797966778278351, 'cross-score': 2.563827},\n",
       " {'corpus_id': 35250, 'score': 0.46669578552246094, 'cross-score': 2.3555288},\n",
       " {'corpus_id': 14728, 'score': 0.4577748775482178, 'cross-score': 2.3106794},\n",
       " {'corpus_id': 20652, 'score': 0.5011643767356873, 'cross-score': 2.2930846},\n",
       " {'corpus_id': 20653, 'score': 0.48917996883392334, 'cross-score': 2.229299},\n",
       " {'corpus_id': 17387, 'score': 0.5642737150192261, 'cross-score': 2.2057667},\n",
       " {'corpus_id': 14802, 'score': 0.5044544339179993, 'cross-score': 2.0563674},\n",
       " {'corpus_id': 39564, 'score': 0.5672644376754761, 'cross-score': 1.676284},\n",
       " {'corpus_id': 14679, 'score': 0.6051740646362305, 'cross-score': 1.5945492},\n",
       " {'corpus_id': 14750, 'score': 0.4836026430130005, 'cross-score': 1.584944},\n",
       " {'corpus_id': 14806, 'score': 0.47698211669921875, 'cross-score': 1.3877895},\n",
       " {'corpus_id': 14821, 'score': 0.46372905373573303, 'cross-score': 1.3640015},\n",
       " {'corpus_id': 5628, 'score': 0.5358618497848511, 'cross-score': 1.2897233},\n",
       " {'corpus_id': 20632, 'score': 0.4619232714176178, 'cross-score': 1.2272785},\n",
       " {'corpus_id': 20638, 'score': 0.4512876570224762, 'cross-score': 1.2227583},\n",
       " {'corpus_id': 14716, 'score': 0.4774951934814453, 'cross-score': 1.0842168},\n",
       " {'corpus_id': 14726, 'score': 0.46023669838905334, 'cross-score': 0.9949666},\n",
       " {'corpus_id': 20711, 'score': 0.4854634702205658, 'cross-score': 0.8184781},\n",
       " {'corpus_id': 9763, 'score': 0.49280524253845215, 'cross-score': 0.72113264},\n",
       " {'corpus_id': 9771, 'score': 0.45408281683921814, 'cross-score': 0.16303732},\n",
       " {'corpus_id': 14805, 'score': 0.5182210206985474, 'cross-score': -0.4363297}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]\n",
    "hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e38258e-5cc2-4b2c-862b-b29827c5077f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msmarco_l6_corpus_ids = [hit['corpus_id'] for hit in hits]\n",
    "len(msmarco_l6_corpus_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d5bcc7b-a8e6-4624-aa21-43b95cadc906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.45471128821372986, cross-score:5.5908050537109375, source:http://arxiv.org/pdf/2204.05862\n",
      "size of 1024 tokens, except for the ‘online’ model described in Section 4.5, where we trained with 2048,\n",
      "which may help stabilize RLHF on long contexts.\n",
      "For both PMP and human feedback ﬁnetuning, we append a special ‘end-of-context’ token at the end of each\n",
      "sample, such that the PM score is predicted directly on top of this token. As explained in Appendix C.4 of\n",
      "[Askell et al., 2021], this appears to improve PM performance.\n",
      "40\n",
      "1071081091010\n",
      "Number of Parameters0.10.20.30.40.50.60.70.8Accuracy\n",
      "Zero-Shot Accuracy on Lambada\n",
      "Plain Language Model\n",
      "RLHF\n",
      "1071081091010\n",
      "Number of Parameters0.30.40.50.60.7Accuracy\n",
      "Zero-Shot Accuracy on ARC-Easy\n",
      "Plain Language Model\n",
      "RLHF\n",
      "1071081091010\n",
      "Number of Parameters0.250.300.350.400.450.500.550.60Accuracy\n",
      "Zero-Shot Accuracy on ARC-Challenge\n",
      "Plain Language Model\n",
      "RLHF\n",
      "1071081091010\n",
      "Number of Parameters0.2500.2750.3000.3250.3500.3750.4000.425Accuracy\n",
      "Zero-Shot Accuracy on MMLU\n",
      "Plain Language Model\n",
      "RLHF\n",
      "-------------------\n",
      "\n",
      "score:0.4980108439922333, cross-score:5.048050880432129, source:http://arxiv.org/pdf/2212.08073\n",
      "from, those appearing in the PM and RL training data. Results are shown in Figure 3, where we compare\n",
      "SL-CAI models and RLHF models. The RLHF models include two types: (1) models trained on only helpfulness data, and (2) models trained on helpfulness and harmlessness. The ﬁgure also includes the RL-CAI (i.e.,\n",
      "RLAIF) models discussed in Section 4. A total of 10,274 helpfulness and 8,135 comparisons were collected\n",
      "for AB testing the 24 snapshots shown collectively in Figures 2 and 3.\n",
      "As expected from prior work, we ﬁnd that the helpful RLHF model is more helpful but also more harmful\n",
      "than HH RLHF. Furthermore, while SL-CAI is less helpful than both RL models, it is more harmless than the\n",
      "helpful RLHF model and more harmful than HH RLHF.8We also compare SL-CAI and pre-trained models\n",
      "in Figure 8, where the 52B-parameter SL-CAI model is shown as the initial snapshot of RL-CAI, while the\n",
      "7These principles were selected in an ad hoc manner for research purposes, and were not carefully designed as in\n",
      "[Glaese et al., 2022]. We have included these principles in Appendix C]\n",
      "8Note that the harmlessness Elo scores for the RLHF models look much closer to together compared to\n",
      "-------------------\n",
      "\n",
      "score:0.499434232711792, cross-score:4.413681507110596, source:http://arxiv.org/pdf/2212.08073\n",
      "feedback model (typically a pretrained LM). Once the desired comparison labels are obtained, the remainder\n",
      "of the training pipeline (i.e., preference model training and RL) is exactly the same as RLHF.\n",
      "We begin by presenting the assistant model with a prompt, and generating a pair of responses. We then\n",
      "present the prompt and response pair to the feedback model with a principle for choosing the more harmless\n",
      "response, in a format like\n",
      "Consider the following conversation between a human and an assistant:\n",
      "[HUMAN/ASSISTANT CONVERSATION]\n",
      "[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
      "Options:\n",
      "(A) [RESPONSE A]\n",
      "(B) [RESPONSE B]\n",
      "The answer is:\n",
      "We then compute the log probability of the responses (A) and(B), and we make a labeled, preference\n",
      "modeling comparison example with the normalized probabilities as targets (and we expect these targets will\n",
      "be fairly well-calibrated [Kadavath et al., 2022], since they are multiple choice responses). We use pre-trained\n",
      "10\n",
      "models for feedback for the experiments in this section, but in Section 2 we also compare against helpful\n",
      "RLHF models in terms of label accuracy on various datasets.\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, hit in enumerate(hits[:3]):\n",
    "    sample = dataset['train'][hit['corpus_id']]\n",
    "    score = hit['score']\n",
    "    print(f\"score:{score}, cross-score:{hit['cross-score']}, source:{sample['source']}\")\n",
    "    print(f\"{sample['chunk']}\")\n",
    "    print('-------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3aa1b0c6-37fe-4c26-b532-79dc6dbf2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('BAAI/bge-reranker-base')\n",
    "cross_inp = [[query, chunks[hit['corpus_id']]] for hit in hits]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeaf1e93-13ca-4150-b1d2-f7eab7e05555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 20638, 'score': 0.4512876570224762, 'cross-score': 0.9574586},\n",
       " {'corpus_id': 17387, 'score': 0.5642737150192261, 'cross-score': 0.9408788},\n",
       " {'corpus_id': 14679, 'score': 0.6051740646362305, 'cross-score': 0.886943},\n",
       " {'corpus_id': 9761, 'score': 0.499434232711792, 'cross-score': 0.871898},\n",
       " {'corpus_id': 39564, 'score': 0.5672644376754761, 'cross-score': 0.80197424},\n",
       " {'corpus_id': 20632, 'score': 0.4619232714176178, 'cross-score': 0.8005458},\n",
       " {'corpus_id': 14725, 'score': 0.5746961832046509, 'cross-score': 0.71381146},\n",
       " {'corpus_id': 9763, 'score': 0.49280524253845215, 'cross-score': 0.6758993},\n",
       " {'corpus_id': 14797, 'score': 0.45471128821372986, 'cross-score': 0.671386},\n",
       " {'corpus_id': 9771, 'score': 0.45408281683921814, 'cross-score': 0.64563036},\n",
       " {'corpus_id': 14680, 'score': 0.4797966778278351, 'cross-score': 0.63509965},\n",
       " {'corpus_id': 14750, 'score': 0.4836026430130005, 'cross-score': 0.6212804},\n",
       " {'corpus_id': 14805, 'score': 0.5182210206985474, 'cross-score': 0.6092513},\n",
       " {'corpus_id': 9755, 'score': 0.4980108439922333, 'cross-score': 0.60406476},\n",
       " {'corpus_id': 14821, 'score': 0.46372905373573303, 'cross-score': 0.6040554},\n",
       " {'corpus_id': 14802, 'score': 0.5044544339179993, 'cross-score': 0.5732974},\n",
       " {'corpus_id': 5628, 'score': 0.5358618497848511, 'cross-score': 0.5458112},\n",
       " {'corpus_id': 14716, 'score': 0.4774951934814453, 'cross-score': 0.43516672},\n",
       " {'corpus_id': 14806, 'score': 0.47698211669921875, 'cross-score': 0.38372642},\n",
       " {'corpus_id': 20711, 'score': 0.4854634702205658, 'cross-score': 0.31397936},\n",
       " {'corpus_id': 20652, 'score': 0.5011643767356873, 'cross-score': 0.30227312},\n",
       " {'corpus_id': 14728, 'score': 0.4577748775482178, 'cross-score': 0.27321604},\n",
       " {'corpus_id': 20653, 'score': 0.48917996883392334, 'cross-score': 0.10840803},\n",
       " {'corpus_id': 35250,\n",
       "  'score': 0.46669578552246094,\n",
       "  'cross-score': 0.010712464},\n",
       " {'corpus_id': 14726,\n",
       "  'score': 0.46023669838905334,\n",
       "  'cross-score': 0.0033747596}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b061a82-d7cb-4629-9fb3-64b4481ea240",
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_corpus_ids = [hit['corpus_id'] for hit in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b21648f-e369-48e1-8eea-8b0010e4a0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.4512876570224762, cross-score:0.9574586153030396, source:http://arxiv.org/pdf/2302.07459\n",
      "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
      "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
      "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
      "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
      "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
      "to control for the amount of RLHF training in the analysis of our experiments.\n",
      "3.2 Experiments\n",
      "3.2.1 Overview\n",
      "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
      "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
      "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
      "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
      "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
      "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
      "-------------------\n",
      "\n",
      "score:0.5642737150192261, cross-score:0.9408788084983826, source:http://arxiv.org/pdf/2302.07842\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "-------------------\n",
      "\n",
      "score:0.6051740646362305, cross-score:0.886942982673645, source:http://arxiv.org/pdf/2204.05862\n",
      "learning from human feedback, which we improve on a roughly weekly cadence. See Section 2.3.\n",
      "4This means that our helpfulness dataset goes ‘up’ in desirability during the conversation, while our harmlessness\n",
      "dataset goes ‘down’ in desirability. We chose the latter to thoroughly explore bad behavior, but it is likely not ideal\n",
      "for teaching good behavior. We believe this difference in our data distributions creates subtle problems for RLHF, and\n",
      "suggest that others who want to use RLHF to train safer models consider the analysis in Section 4.4.\n",
      "5\n",
      "1071081091010\n",
      "Number of Parameters0.20.30.40.50.6Mean Eval Acc\n",
      "Mean Zero-Shot Accuracy\n",
      "Plain Language Model\n",
      "RLHF\n",
      "1071081091010\n",
      "Number of Parameters0.20.30.40.50.60.7Mean Eval Acc\n",
      "Mean Few-Shot Accuracy\n",
      "Plain Language Model\n",
      "RLHFFigure 3 RLHF model performance on zero-shot and few-shot NLP tasks. For each model size, we plot\n",
      "the mean accuracy on MMMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, and\n",
      "TriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, hit in enumerate(hits[:3]):\n",
    "    sample = dataset['train'][hit['corpus_id']]\n",
    "    score = hit['score']\n",
    "    print(f\"score:{score}, cross-score:{hit['cross-score']}, source:{sample['source']}\")\n",
    "    print(f\"{sample['chunk']}\")\n",
    "    print('-------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ce1a732-f87b-4920-a1d5-23182a7a660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1. bi-encoder 14679, cross-encoder 14797, bge 20638\n",
      "top 2. bi-encoder 14725, cross-encoder 9755, bge 17387\n",
      "top 3. bi-encoder 39564, cross-encoder 9761, bge 14679\n",
      "top 4. bi-encoder 17387, cross-encoder 14725, bge 9761\n",
      "top 5. bi-encoder 5628, cross-encoder 14680, bge 39564\n",
      "top 6. bi-encoder 14805, cross-encoder 35250, bge 20632\n",
      "top 7. bi-encoder 14802, cross-encoder 14728, bge 14725\n",
      "top 8. bi-encoder 20652, cross-encoder 20652, bge 9763\n",
      "top 9. bi-encoder 9761, cross-encoder 20653, bge 14797\n",
      "top 10. bi-encoder 9755, cross-encoder 17387, bge 9771\n",
      "top 11. bi-encoder 9763, cross-encoder 14802, bge 14680\n",
      "top 12. bi-encoder 20653, cross-encoder 39564, bge 14750\n",
      "top 13. bi-encoder 20711, cross-encoder 14679, bge 14805\n",
      "top 14. bi-encoder 14750, cross-encoder 14750, bge 9755\n",
      "top 15. bi-encoder 14680, cross-encoder 14806, bge 14821\n",
      "top 16. bi-encoder 14716, cross-encoder 14821, bge 14802\n",
      "top 17. bi-encoder 14806, cross-encoder 5628, bge 5628\n",
      "top 18. bi-encoder 35250, cross-encoder 20632, bge 14716\n",
      "top 19. bi-encoder 14821, cross-encoder 20638, bge 14806\n",
      "top 20. bi-encoder 20632, cross-encoder 14716, bge 20711\n",
      "top 21. bi-encoder 14726, cross-encoder 14726, bge 20652\n",
      "top 22. bi-encoder 14728, cross-encoder 20711, bge 14728\n",
      "top 23. bi-encoder 14797, cross-encoder 9763, bge 20653\n",
      "top 24. bi-encoder 9771, cross-encoder 9771, bge 35250\n",
      "top 25. bi-encoder 20638, cross-encoder 14805, bge 14726\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(f\"top {i+1}. bi-encoder {retrival_corpus_ids[i]}, cross-encoder {msmarco_l6_corpus_ids[i]}, bge {bge_corpus_ids[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc9fd0-d7d5-4f0b-915a-047a4410f2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
