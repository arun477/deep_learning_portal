{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953dfd31",
   "metadata": {},
   "source": [
    "https://github.com/TaoRuijie/ECAPA-TDNN/blob/main/model.py#L132 (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 8]), torch.Size([1, 64, 5]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.AdaptiveAvgPool1d(5)\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb670d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 16, 50]), torch.Size([20, 33, 24]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2, padding=0)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aa452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        ) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.se(input)\n",
    "        return  input * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 16, 50]), torch.Size([20, 16, 50]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SEModule(16)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcce1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottle2neck(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=None, dilation=None, scale=8):\n",
    "        super().__init__()\n",
    "        width = int(math.floor(planes/scale))\n",
    "        self.conv1 = nn.Conv1d(inplanes, width*scale, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(width*scale)\n",
    "        self.nums = scale - 1\n",
    "        convs = []\n",
    "        bns = []\n",
    "        num_pad = math.floor(kernel_size/2)*dilation\n",
    "        \n",
    "        for i in range(self.nums):\n",
    "            convs.append(nn.Conv1d(width, width, kernel_size=kernel_size, dilation=dilation, padding=num_pad))\n",
    "            bns.append(nn.BatchNorm1d(width))\n",
    "            \n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "        self.conv3 = nn.Conv1d(width*scale, planes, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(planes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.width = width\n",
    "        self.se = SEModule(planes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i==0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "                \n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(sp)\n",
    "            sp = self.bns[i](sp)\n",
    "            \n",
    "            if i==0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "        out = torch.cat((out, spx[self.nums]), 1)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out) \n",
    "        \n",
    "        out = self.se(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreEmphasis(nn.Module):\n",
    "    def __init__(self, coef:float=0.97):\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        self.register_buffer(\n",
    "            'flipped_filter', torch.tensor([-self.coef, 1.]).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        input = F.pad(input, (1,0), 'reflect')\n",
    "        return F.conv1d(input, self.flipped_filter).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbankAug(nn.Module):\n",
    "    def __init__(self, freq_mask_width=(0,8), time_mask_width=(0, 10)):\n",
    "        super().__init__()\n",
    "        self.time_mask_width = time_mask_width\n",
    "        self.freq_mask_width = freq_mask_width\n",
    "    \n",
    "    def mask_along_axis(self, x, dim):\n",
    "        original_size = x.shape\n",
    "        batch, fea, time = x.shape\n",
    "        if dim==1:\n",
    "            D = fea\n",
    "            width_range = self.freq_mask_width\n",
    "        else:\n",
    "            D = time\n",
    "            width_range = self.time_mask_width\n",
    "            \n",
    "        mask_len = torch.randint(width_range[0], width_range[1], (batch, 1), device=x.device).unsqueeze(2)\n",
    "        mask_pos = torch.randint(0, max(1, D-max_len.max()), (batch, 1), device=x.device).unsqueeze(2)\n",
    "        \n",
    "        arange = torch.arange(D, device=x.device).view(1, 1, -1)\n",
    "        mask = (mask_pos <= arange) * (arange < (mask_pos+mask_len))\n",
    "        mask = mask.any(dim=1)\n",
    "        \n",
    "        if dim==1:\n",
    "            mask = mask.unsqueeze(2)\n",
    "        else:\n",
    "            mask = mask.unsqueeze(2)\n",
    "        \n",
    "        x = x.masked_fill_(mask,0.0)\n",
    "        \n",
    "        return x.view(*original_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mask_along_axis(x, dim=2)\n",
    "        x = self.mask_along_axis(x, dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02751e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc05613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
