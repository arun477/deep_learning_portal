{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7edcf4ad",
   "metadata": {},
   "source": [
    "https://github.com/TaoRuijie/ECAPA-TDNN/blob/main/model.py#L132 (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc8f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 8]), torch.Size([1, 64, 5]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.AdaptiveAvgPool1d(5)\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49695f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 16, 50]), torch.Size([20, 33, 24]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2, padding=0)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60787d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        ) \n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.se(input)\n",
    "        return  input * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ee0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 16, 50]), torch.Size([20, 16, 50]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SEModule(16)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottle2neck(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=None, dilation=None, scale=8):\n",
    "        super().__init__()\n",
    "        width = int(math.floor(planes/scale))\n",
    "        self.conv1 = nn.Conv1d(inplanes, width*scale, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(width*scale)\n",
    "        self.nums = scale - 1\n",
    "        convs = []\n",
    "        bns = []\n",
    "        num_pad = math.floor(kernel_size/2)*dilation\n",
    "        \n",
    "        for i in range(self.nums):\n",
    "            convs.append(nn.Conv1d(width, width, kernel_size=kernel_size, dilation=dilation, padding=num_pad))\n",
    "            bns.append(nn.BatchNorm1d(width))\n",
    "            \n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "        self.conv3 = nn.Conv1d(width*scale, planes, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(planes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.width = width\n",
    "        self.se = SEModule(planes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i==0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "                \n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(sp)\n",
    "            sp = self.bns[i](sp)\n",
    "            \n",
    "            if i==0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "        out = torch.cat((out, spx[self.nums]), 1)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out) \n",
    "        \n",
    "        out = self.se(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreEmphasis(nn.Module):\n",
    "    def __init__(self, coef:float=0.97):\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        self.register_buffer(\n",
    "            'flipped_filter', torch.tensor([-self.coef, 1.]).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.tensor):\n",
    "        input = input.unsqueeze(1)\n",
    "        input = F.pad(input, (1,0), 'reflect')\n",
    "        return F.conv1d(input, self.flipped_filter).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbankAug(nn.Module):\n",
    "    def __init__(self, freq_mask_width=(0,8), time_mask_width=(0, 10)):\n",
    "        super().__init__()\n",
    "        self.time_mask_width = time_mask_width\n",
    "        self.freq_mask_width = freq_mask_width\n",
    "    \n",
    "    def mask_along_axis(self, x, dim):\n",
    "        original_size = x.shape\n",
    "        batch, fea, time = x.shape\n",
    "        if dim==1:\n",
    "            D = fea\n",
    "            width_range = self.freq_mask_width\n",
    "        else:\n",
    "            D = time\n",
    "            width_range = self.time_mask_width\n",
    "            \n",
    "        mask_len = torch.randint(width_range[0], width_range[1], (batch, 1), device=x.device).unsqueeze(2)\n",
    "        mask_pos = torch.randint(0, max(1, D-max_len.max()), (batch, 1), device=x.device).unsqueeze(2)\n",
    "        \n",
    "        arange = torch.arange(D, device=x.device).view(1, 1, -1)\n",
    "        mask = (mask_pos <= arange) * (arange < (mask_pos+mask_len))\n",
    "        mask = mask.any(dim=1)\n",
    "        \n",
    "        if dim==1:\n",
    "            mask = mask.unsqueeze(2)\n",
    "        else:\n",
    "            mask = mask.unsqueeze(2)\n",
    "        \n",
    "        x = x.masked_fill_(mask,0.0)\n",
    "        \n",
    "        return x.view(*original_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mask_along_axis(x, dim=2)\n",
    "        x = self.mask_along_axis(x, dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECAPA_TDNN(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.trochfbank = nn.Sequential(\n",
    "            PreEmphasis(),\n",
    "            torchaudio.transforms.MelSpectrograml(sample_rate=16000, n_fft=512, win_length=400,\n",
    "                                                 hop_length=160, f_min=20, f_max=7600,\n",
    "                                                 window_fn=torch.hamming_window, n_mels=80)\n",
    "        )\n",
    "        \n",
    "        self.specaug = FbankAug()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(80, C, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(C)\n",
    "        \n",
    "        self.layer1 = Bottle2neck(C, C, kernel_size=3, dilation=2, scale=8)\n",
    "        self.layer2 = Bottle2neck(C, C, kernel_size=3, dilation=3, scale=8)\n",
    "        self.layer3 = Bottle2neck(C, C, kernel_size=3, dilation=4, scale=8)     \n",
    "        self.layer4 = nn.Conv1d(3*C, 1536, kernel_size=1)\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(4608, 256, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(256, 1536, kernel_size=1),\n",
    "            nn.Softmax(dim=2)\n",
    "        )\n",
    "        \n",
    "        self.bn5 = nn.BatchNorm1d(3072)\n",
    "        self.fc6 = nn.Linear(3072, 192)\n",
    "        self.bn6 = nn.BatchNorm1d(192)\n",
    "    \n",
    "    def forward(self, x, aug):\n",
    "        with torch.no_grad():\n",
    "            x = self.trochfbank(x) + 1e-6\n",
    "            x = x.log()\n",
    "            x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "            if aug == True:\n",
    "                x = self.specaug(x)\n",
    "            \n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "            x1 = self.layer1(x)\n",
    "            x2 = self.layer2(x+x1)\n",
    "            x3 = self.layer3(x+x1+x2)\n",
    "            \n",
    "            x = self.layer4(torch.cat(x1,x2,x3), dim=1)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            t = x.size()[-1]\n",
    "            global_x = torch.cat((x,torch.mean(x,dim=2,keepdim=True).repeat(1,1,t),\n",
    "                                 torch.sqrt(torch.var(x, dim=2, keepdim=True).clamp(min=1e-4)).repeat(1,1,t)),\n",
    "                                dim=1)\n",
    "            w = self.attention(global_x)\n",
    "            \n",
    "            mu = torch.sum(x*w, dim=2)\n",
    "            sg = torch.sqrt( (torch.sum((x**2)*w, dim=2)-mu**2).clamp(min=1e-4) )\n",
    "            \n",
    "            x = torch.cat((mu,sg),1)\n",
    "            x = self.bn5(x)\n",
    "            x = self.fc6(x)\n",
    "            x = self.bn6(x)\n",
    "            \n",
    "            return  x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fa222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a90f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
