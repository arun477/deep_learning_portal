{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2af0f4",
   "metadata": {},
   "source": [
    "https://github.com/AnswerDotAI/fsdp_qlora/blob/main/train.py (ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install hqq\n",
    "# !pip install wandb\n",
    "# !pip install datasets\n",
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2152b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "import types\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "import copy\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "try:\n",
    "    from hqq.core.quantize import HQQLinear, HQQBackend, BaseQuantizeConfig\n",
    "except ImportError:\n",
    "    HQQLinear = None\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, args, log_to='stdout', project_name='fsdp_qlora',\n",
    "                 entity=None, group=None, name=None, rank=0):\n",
    "        self.log_to = log_to\n",
    "        if self.log_to == 'wandb' and rank==0:\n",
    "            import wandb\n",
    "            wandb.init(\n",
    "                project=project_name,\n",
    "                entity=entity,\n",
    "                group=group,\n",
    "                name=name,\n",
    "                cofig=args\n",
    "            )\n",
    "\n",
    "    def log(self, d:Dict, rank:int):\n",
    "        if rank != 0:\n",
    "            return\n",
    "        if self.log_to == \"tqdm\":\n",
    "            for k,v in d.items():\n",
    "                tqdm.write(f'{k}: {v}')\n",
    "        elif self.log_to == 'wandb':\n",
    "            wandb.log(d)\n",
    "        elif self.log_to == 'stdout':\n",
    "            for k,v in d.item():\n",
    "                print(f'{k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress_bar(progress_bar:tqdm, epoch:int, log_loss:float, log_lr:float, rank:int):\n",
    "    if rank==0:\n",
    "        if log_lr >= 0:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}, lr {log_lr:.2e}\", refresh=True)\n",
    "        else:\n",
    "            progress_bar.set_description(f\"epoch {epoch}, loss {log_loss:.3f}\", refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:Union[dict,None]=None,\n",
    "                  skip_modules:List[str]=['lm_head'], **kwargs):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, nn.Linear) and name not in skip_modules:\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            elif issubclass(linear_replacement, HQQLinear):\n",
    "                model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f78096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(\n",
    "    nn.Linear(5, 10),\n",
    "    nn.Sequential(nn.Linear(10, 50), nn.ReLU())\n",
    ")\n",
    "# m = m.to('cuda')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.float32 True cpu\n",
      "torch.Size([10]) torch.float32 True cpu\n",
      "torch.Size([50, 10]) torch.float32 True cpu\n",
      "torch.Size([50]) torch.float32 True cpu\n"
     ]
    }
   ],
   "source": [
    "for p in m.parameters():\n",
    "    print(p.shape, p.dtype, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ff371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear4bit(in_features=5, out_features=10, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear4bit(in_features=10, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = replace_linear(m, linear_replacement=Linear4bit)\n",
    "# r = r.to('cuda')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfe736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.float32 False cpu\n",
      "torch.Size([10]) torch.float32 True cpu\n",
      "torch.Size([50, 10]) torch.float32 False cpu\n",
      "torch.Size([50]) torch.float32 True cpu\n"
     ]
    }
   ],
   "source": [
    "for p in r.parameters():\n",
    "    print(p.shape, p.dtype, p.requires_grad, p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4BYywc_oAEk5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantized_meta_for_peft(model:nn.Module):\n",
    "  def temp_to_method(self, *args, **kwargs):\n",
    "    return self\n",
    "  for param in model.parameters():\n",
    "    if isinstance(param, Params4bit):\n",
    "      param.quant_state._orig_to = param.quant_state.to\n",
    "      param.quant_state.to = types.MethodType(temp_to_method, param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJUj0tpGCKXf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantized_peft_meta_for_training(model:nn.Module):\n",
    "  for param in model.parameters():\n",
    "    if isinstance(param, Params4bit) and hasattr(param.quant_state, '_orig_to'):\n",
    "      param.quant_state.to = param.quant_state._orig_to\n",
    "      param.quant_state._orig_to = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r8XKqV6FEwt5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_key: nn\n",
      "value_key: Conv2d\n",
      "_ .\n"
     ]
    }
   ],
   "source": [
    "name = \"nn.Conv2d\"\n",
    "module_key, _, value_key = name.rpartition('.')\n",
    "\n",
    "print(\"module_key:\", module_key)\n",
    "print(\"value_key:\", value_key)\n",
    "print('_', _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZJO3ykWC2-O",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False, quant_method:str='bnb'):\n",
    "\n",
    "  def place_on_device(value):\n",
    "    if is_meta_rank:\n",
    "      device = 'meta'\n",
    "    elif low_memory:\n",
    "      device = 'cpu'\n",
    "    return value.to(device=device)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "      if verbose:\n",
    "        print(f'skipping {name} because it is in skip_names')\n",
    "      return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "      submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "      print(f'module {module_key} not found:\\n{e}')\n",
    "      return\n",
    "\n",
    "    try:\n",
    "      if quant_method=='bnb':\n",
    "          param = submodule.get_parameter(value_key)\n",
    "          if isinstance(param, Params4bit):\n",
    "            value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "            if is_meta_rank:\n",
    "              value = type(param)(value.data.to('meta'), **value.__dict__)\n",
    "            elif low_memory:\n",
    "              value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "          else:\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "      elif quant_method=='hqq':\n",
    "        if isinstance(submodule, HQQLinear):\n",
    "          if value_key == 'weight':\n",
    "            submodule.linear_layer.to_empty(device=device)\n",
    "            submodule.linear_layer.weight.data.copy_(value.to(device=device, dtype=dtype))\n",
    "            submodule.initialize()\n",
    "\n",
    "            if is_meta_rank:\n",
    "              setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to('meta')))\n",
    "            elif low_memory:\n",
    "              setattr(submodule, \"W_q\", nn.Parameter(submodule.W_q.to(\"cpu\")))\n",
    "            submodule.in_gpu = False\n",
    "\n",
    "          if value_key == \"bias\":\n",
    "            raise ValueError('bias not supported in HQQLinear yet')\n",
    "        else:\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            value = type(param)(place_on_device(value).data)\n",
    "\n",
    "    except AttributeError:\n",
    "      # SRC: remove pass\n",
    "      value = place_on_device(value)\n",
    "\n",
    "    if HQQLinear is None or not isinstance(submodule, HQQLinear):\n",
    "      setattr(submodule, value_key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19TSiUWJaL22",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### instruction:\\n{instruction}\\n\\n### input:\\n{input}\\n\\n### response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"below is an instruction that describes a task. \"\n",
    "        \"write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### instrucion:\\n{instruction}\\n\\n### response:\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4-ZKcN9Ia2x5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "  def __init__(self, dataset, tokenizer, style='alpaca'):\n",
    "    self.dataset = dataset\n",
    "    self.tokenizer = tokenizer\n",
    "    self.style = style\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    IGNORE_INDEX = -100\n",
    "    if self.style == 'guanaco':\n",
    "      prompt = self.dataset[index]['text'].split('### Assistant: ')[0]\n",
    "      example = self.dataset[index]['text']\n",
    "    elif self.style == 'qna':\n",
    "      prompt_template = \"###Context:\\n{context}\\n###Question:\\n{question}\\n###Answer:\\n\"\n",
    "      sample = self.dataset[index]\n",
    "      prompt = prompt_template.format_map(sample)\n",
    "      example = prompt + sample['answer']\n",
    "    else: #alpaca\n",
    "      ann = self.dataset[index]\n",
    "      if ann.get(\"input\", \"\") == \"\":\n",
    "        prompt = PROMPT_DICT['prompt_no_input'].format_map(ann)\n",
    "      else:\n",
    "        prompt = PROMPT_DICT['prompt_input'].format_map(ann)\n",
    "      example = prompt + ann['output']\n",
    "\n",
    "    prompt = torch.tensor(\n",
    "        self.tokenizer.encode(prompt), dtype=torch.int64\n",
    "    )\n",
    "    example = self.tokenizer.encode(example)\n",
    "    example.append(self.tokenizer.eos_token_id)\n",
    "    example = torch.tensor(\n",
    "        example, dtype=torch.int64\n",
    "    )\n",
    "    labels = copy.deepcopy(example)\n",
    "    labels[: len(prompt)] = -1\n",
    "    example_mask = example.ge(0)\n",
    "    label_mask = labels.ge(0)\n",
    "    example[~example_mask] = 0\n",
    "    labels[~label_mask] = IGNORE_INDEX\n",
    "\n",
    "    return  {\n",
    "        'input_ids': example.tolist(),\n",
    "        'labels': labels.tolist(),\n",
    "        'attention_mask': example_mask.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L2y-PWtjfzSk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(tokenizer:PreTrainedTokenizerFast, args:Dict):\n",
    "  from datasets import Dataset, load_dataset\n",
    "\n",
    "  if args['dataset'] == 'alpaca':\n",
    "    dataset = load_dataset('yahma/alpaca-cleaned')['train']\n",
    "  elif args['dataset'] == 'alpaca_sample':\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split='train[:512]')\n",
    "  elif args['dataset'] == 'dummy':\n",
    "    dataset = Dataset.from_dict({\n",
    "        'instruction': [\"instruction\"]*512,\n",
    "        'input': [\"input\"]*512,\n",
    "        'output': ['output'*10000]*512\n",
    "    })\n",
    "  elif args['dataset'] == 'guanaco':\n",
    "    dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split='train')\n",
    "  elif args['dataset'] == 'sql':\n",
    "    dataset = load_dataset(\"knowrohit07/know_sql\")['validation']\n",
    "    dataset = dataset.shuffle(seed=args['seed'])\n",
    "    dataset = dataset.select(range(1000,len(dataset)))\n",
    "\n",
    "  dataset = dataset.select(range(0, len(dataset)-len(dataset)%(args['batch_size']*args['gradient_accumulation_steps'])))\n",
    "\n",
    "  if args['dataset'] == 'guanaco':\n",
    "    dataset = InstructionDataset(dataset, tokenizer, style='guanaco')\n",
    "  elif args['dataset'] == 'sql':\n",
    "    dataset = InstructionDataset(dataset, tokenizer, style='qna')\n",
    "  else: #alpaca\n",
    "    dataset = InstructionDataset(dataset, tokenizer, style='alpaca')\n",
    "\n",
    "  def collate_fn(batch, with_attention_mask=False):\n",
    "    input_ids  = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_masks = [torch.tensor(item['attension_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)[:,:args['context_length']]\n",
    "    if with_attention_mask:\n",
    "      attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)[:,:args['context_length']]\n",
    "    else:\n",
    "      attention_masks = None\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)[:,:args['context_length']]\n",
    "    return  {\n",
    "        'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels\n",
    "    }\n",
    "\n",
    "  sampler = DistributedSampler(dataset, seed=args['seed'])\n",
    "\n",
    "  dataloader = DataLoader(dataset, batch_size=args['batch_size'], collate_fn=collate_fn, sampler=sampler)\n",
    "\n",
    "  return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vKKX8rrSf9H4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def _get_cosine_one_cycle_lr_lambda(\n",
    "    current_step:int, *, num_warmup_steps:int, num_training_steps:int, min_lr_fraction=0.1\n",
    ") :\n",
    "    if current_step < num_warmup_steps:\n",
    "        return  float(current_step) / float(max(1, num_warmup_steps))\n",
    "    scale_term = (1-min_lr_fraction)\n",
    "    progress = float(current_step-num_warmup_steps) / float(max(1, num_training_steps-num_warmup_steps))\n",
    "    return (math.cos(math.pi*progress)+1) * 0.5 * scale_term + min_lr_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import functools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_cosine_one_cycle_scheduler(optimizer:optim.Optimizer, num_warmup_steps:int,\n",
    "                                  num_training_steps:int, min_lr_fraction:float=0.1):\n",
    "    lr_lambda = functools.partial(\n",
    "        _get_cosine_one_cycle_lr_lambda,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        min_lr_fraction=min_lr_fraction\n",
    "    ) \n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "def get_lr_scheduler(optimizer:optim.Optimizer, dataloader:DataLoader,\n",
    "                    gradient_accumulation_steps:int, args:Dict):\n",
    "    num_training_steps = args['num_epochs'] * len(dataloader) // gradient_accumulation_steps\n",
    "    num_warmup_steps = int(num_training_steps*0.1)\n",
    "    if args['lr_scheduler'] == 'linear':\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    elif args['lr_scheduler'] == 'cosine':\n",
    "        lr_scheduler = get_cosine_one_cycle_scheduler(optimizer, num_warmup_steps, num_training_steps,\\\n",
    "                                                     min_lr_fraction=0.1)\n",
    "    elif args['lr_scheduler'] == 'constant':\n",
    "        lr_scheduler = None\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{args['lr_scheduler']} lr scheduler not implemented yet\")\n",
    "    return lr_scheduler, num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model:nn.Module, args:Dict):\n",
    "    if args['optimizer'] == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    elif args['optimizer'] == 'sgd':\n",
    "        return optim.SGD(model.parameters(), lr=args['lr'])\n",
    "    elif args['optimizer'] == 'adadelta':\n",
    "        return optim.Adadelta(model.parameters(), lr=args['lr'])\n",
    "    elif args['optimizer'] == 'adamw':\n",
    "        #SRC: not torch.optim just optim\n",
    "        return optim.AdamW(model.parameters(), lr=args['lr'], betas=(0.9, 0.95),\n",
    "                          eps=1e-5, weight_decay=args['wd'])\n",
    "    else:\n",
    "        return ValueError('invalid optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LLAMA_ATTENTION_CLASSES, LlamaMLP, LlamaDecoderLayer\n",
    "from torch.distributed.fsdp.wrap import _or_policy, lambda_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "from peft.tuners import PrefixEncoder, PromptEncoder, PromptEmbedding\n",
    "\n",
    "def get_wrapping_policy(custom_policy:bool=False):\n",
    "    if custom_policy:\n",
    "        def lambda_policy_fn(module):\n",
    "            return (isinstance(module, nn.Sequential) and all(m.weight.requires_grad for m in module))\n",
    "    else:\n",
    "        def lambda_policy_fn(module):\n",
    "            return (\n",
    "                len(list(module.named_children()))==0\n",
    "                and getattr(module, 'weight', None) is not None\n",
    "                and module.weight.requires_grad\n",
    "            )\n",
    "    \n",
    "    def self_attn_policy_fn(module):\n",
    "        return isinstance(module, tuple(LLAMA_ATTENTION_CLASSES.values()))\n",
    "    \n",
    "    def mlp_policy_fn(module):\n",
    "        return isinstance(module, LlamaMLP)\n",
    "    \n",
    "    lambda_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=lambda_policy_fn) \n",
    "    self_attn_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=self_attn_policy_fn)\n",
    "    mlp_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=mlp_policy_fn)\n",
    "    transformer_layer_name = LlamaDecoderLayer\n",
    "    transformer_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls=(\n",
    "            PrefixEncoder,\n",
    "            PromptEncoder,\n",
    "            PromptEmbedding,\n",
    "            transformer_layer_name\n",
    "        ) \n",
    "    )\n",
    "    policies = [lambda_policy, transformer_wrap_policy]\n",
    "    if custom_policy:\n",
    "        policies.extend([self_attn_policy, mlp_policy])\n",
    "    return functools.partial(_or_policy, policies=policies) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LORA(nn.Module):\n",
    "    #SRC: type is missing here\n",
    "    def __init__(self, base_layer, lora_rank:int, lora_alpha:float, lora_dropout:float):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        dtype = getattr(base_layer, 'compute_dtype', next(base_layer.paramteres()).dtype)\n",
    "        device = next(base_layer.paramteres()).device\n",
    "        lora_A = nn.Linear(base_layer.in_features, lora_rank, bias=False, device=device, dtype=dtype)\n",
    "        lora_B = nn.Linear(lora_rank, base_layer.out_features, bias=False, device=device, dtype=dtype)\n",
    "        lora_B.weight.data.zero_()\n",
    "        \n",
    "        self.lora_AB = nn.Sequential(lora_A, lora_B)\n",
    "        \n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "        self.scaling = self.lora_alpha/lora_rank\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, *args, **kwargs):\n",
    "        result = self.base_layer(x, *args, **kwargs)\n",
    "        result = result.clone()\n",
    "        \n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            expected_dtype = result.dtype\n",
    "            x = x.to(next(iter(self.lora_B)).weight.dtype)\n",
    "        \n",
    "        output = self.loraAB(self.lora_dropout(x))\n",
    "        if requires_conversion:\n",
    "            output = output.to(expected_dtype)\n",
    "        output = output * self.scaling\n",
    "        \n",
    "        result += output\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511edc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
