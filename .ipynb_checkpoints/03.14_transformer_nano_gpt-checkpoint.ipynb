{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de686a06",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.03762 (original paper) <br/>\n",
    "https://jalammar.github.io/illustrated-transformer/ (blog illustrated explanation) <br/>\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html (paper with annotation with code) <br/>\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY (karpathy gpt from scratch video) <br/>\n",
    "https://github.com/karpathy/ng-video-lecture (karpathy gpt from scratch notebooks) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94e4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85399596",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384 # embedding dim\n",
    "n_head = 6 # dim of key, query, value, embedding\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112eae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd77b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tinyshakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f10506",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l : \"\".join(itos[c] for c in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15353a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baacdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    ys = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    xs, ys = xs.to(device), ys.to(device)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "770df152",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    o = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        o[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6870f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention single head.\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(n_embd, n_head, bias=False)\n",
    "        self.query = nn.Linear(n_embd, n_head, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_head, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # .register_buffer: to store non-trainable tensors.\n",
    "        # torch.tril will create lower triangle matrix.\n",
    "        # used for masking future chars. \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: B,T,C\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        # get the key embedding of each token in the context.\n",
    "        k = self.key(x) # shape: B,T,h\n",
    "        \n",
    "        # get the query embedding of each token in the context.\n",
    "        q = self.query(x) # shape: B,T,h\n",
    "        \n",
    "        # calc dot product each query embedding with key embedding, this will give\n",
    "        # sort of 'affinitity or attention' score for each token against each other token.\n",
    "        # k.shape[-1]**0.5 : this is kind of initialization, graident stabilzing factor, k.shape[-1] is the\n",
    "        # head size (n_head), this suggested scaling factor as per paper.\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # shape: B,T,h -> B,T,T \n",
    "        \n",
    "        # masking of future token. e.g first can only see itself, and second one see itself and prev\n",
    "        # one.\n",
    "        # float('-inf'): because during exponential those will go to zero.\n",
    "        # todo: the need for tril[:T, :T] not clear yet because tril already defined in T shae???\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # shape: B,T,T\n",
    "        \n",
    "        # convert wei is probs\n",
    "        wei = F.softmax(wei, dim=-1) # shape: B,T,T\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # get the value embedding of each token.\n",
    "        v = self.value(x) # shape: B,T,h\n",
    "        \n",
    "        # multiply the probs score to each token value embedding to get the weighted embedding value.\n",
    "        # todo: review this process reasoning.\n",
    "        out = wei @ v # shape: B,T,T @ B,T,h -> B,T,h\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4436f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2b114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4c684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8a20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb92c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d38a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
