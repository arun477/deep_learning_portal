{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8a84da",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2305.14314 (original paper) <br/>\n",
    "https://github.com/artidoro/qlora (qlora official github repo) <br/>\n",
    "https://github.com/TimDettmers/bitsandbytes (bitsandbytes github 8-bit quantisation technique) <br/>\n",
    "https://github.com/huggingface/peft (hugging face fine-tuning lib) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc4df753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284db41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic idea is quantise the original model weights with low bits than original bits for reducing \n",
    "# original weight memory, and use the lora technique to fine tune the down stream tasks without updating \n",
    "# the pre trained weights.\n",
    "\n",
    "# why this is exciting idea?\n",
    "# LLaMA 65B requires ~780GB GPU memory.ðŸ¥µ\n",
    "# same model with qlora techniques can run on ~48GB single GPU memory.\n",
    "# can be trained within 24 hrs time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885af394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data type:  torch.float32\n",
      "x memory: 381 MB\n",
      "y data type:  torch.int8\n",
      "y memory: 95 MB\n"
     ]
    }
   ],
   "source": [
    "# todo: name may be different need to check\n",
    "# todo: example need to be verified\n",
    "# linear quantisation:\n",
    "# naive 8bit int quantisation from float32 bit.\n",
    "# element_size: returns bytes of each element in the tensor.\n",
    "# numel: returns number of elements.\n",
    "# memory is reduced drastically.\n",
    "# at the same time distribution is sort of intact.\n",
    "\n",
    "x = torch.randn(100000000) * 50\n",
    "memory_of_x = round((x.element_size() * x.numel())/(1024**2))\n",
    "print('x data type: ', x.dtype)\n",
    "print(f\"x memory: {memory_of_x} MB\")\n",
    "abs_max_x = x.max().abs()\n",
    "\n",
    "y = (127 * (x/abs_max_x)).round().to(torch.int8)\n",
    "memory_of_y = round((y.element_size() * y.numel())/(1024*1024))\n",
    "print('y data type: ', y.dtype)\n",
    "print(f\"y memory: {memory_of_y} MB\")\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(10, 2))\n",
    "for title, ax, data in zip(('float32 (x)', 'int8 (y)'), axs, (x, y)):\n",
    "    ax.hist(data.numpy(), bins=32)\n",
    "    ax.set_title(title)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure case when there are outliers. then this technique dosn't properly preserve the distributions.\n",
    "# white strip is the missing bins this happens due to this outlier and eneven property of the input data.\n",
    "\n",
    "# big outlier issue: normal distribution (scaled with 5) is concated with extreme outlier value of 1000.\n",
    "x = torch.cat([torch.randn(99999990) * 50, torch.tensor([1000.0, -1000.0])])\n",
    "\n",
    "# narrow range issue: this is narrow range issue example.\n",
    "# x = torch.randn(100000000) * 0.01\n",
    "\n",
    "memory_of_x = round((x.element_size() * x.numel())/(1024**2))\n",
    "print('x data type: ', x.dtype)\n",
    "print(f\"x memory: {memory_of_x} MB\")\n",
    "abs_max_x = x.max().abs()\n",
    "\n",
    "y = (127 * (x/abs_max_x)).round().to(torch.int8)\n",
    "memory_of_y = round((y.element_size() * y.numel())/(1024*1024))\n",
    "print('y data type: ', y.dtype)\n",
    "print(f\"y memory: {memory_of_y} MB\")\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "for title, ax, data in zip(('float32 (x)', 'int8 (y)'), axs, (x, y)):\n",
    "    ax.hist(data.numpy(), bins=500)\n",
    "    ax.set_title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block-wise k-bit quantisation:\n",
    "# split the input into buckets and run the naive quantisation on those bucket,\n",
    "# this will help to side step from the outlier issues.\n",
    "\n",
    "def quantise(x):\n",
    "    abs_max_x = x.max().abs()\n",
    "    return (127 * (x/abs_max_x)).round().to(torch.int8)\n",
    "\n",
    "block_size = 10000\n",
    "x = torch.cat([torch.randn(99999990) * 50, torch.tensor([1000.0, -1000.0])])\n",
    "print('x data type: ', x.dtype)\n",
    "print(f\"x memory: {memory_of_x} MB\")\n",
    "\n",
    "x_chunked_lst = x.chunk(block_size)\n",
    "x_chunked_lst_quantised = map(quantise, x_chunked_lst)\n",
    "\n",
    "y = torch.cat(list(x_chunked_lst_quantised))\n",
    "memory_of_y = round((y.element_size() * y.numel())/(1024*1024))\n",
    "print('y data type: ', y.dtype)\n",
    "print(f\"y memory: {memory_of_y} MB\")\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "for title, ax, data in zip(('float32 (x)', 'int8 (y)'), axs, (x, y)):\n",
    "    ax.hist(data.numpy(), bins=500)\n",
    "    ax.set_title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0a0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b6ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
