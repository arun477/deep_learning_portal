{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ae5b7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74318780",
   "metadata": {},
   "source": [
    "https://dl.acm.org/doi/pdf/10.5555/3454287.3455397#:~:text=RMSNorm%20regularizes%20the%20summed%20inputs,thus%20more%20efficient%20than%20LayerNorm. (rms norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861ca54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2d43e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner*2, bias=args.bias)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            kernel_size=args.d_conv,\n",
    "            bias=args.conv_bias,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv-1\n",
    "        )\n",
    "        \n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.dt_state*2, bias=False)\n",
    "        \n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "        \n",
    "        A = repeat(torch.arange(1, args.d_state+1), 'n -> d n', d=args.d_inner)\n",
    "        \n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        \n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_inner, args.d_model, bias=args.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b416b",
   "metadata": {},
   "source": [
    "N,C,L = batch size, channels, sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "327a4d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 5)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ed4639b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [1, 2, 3, 4],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat(x, 'n -> 3 n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f35f90c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat(x, 'n -> 3 n').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a4571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
