{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea113677",
   "metadata": {
    "id": "ea113677"
   },
   "source": [
    "https://arxiv.org/abs/1706.03762 (original transformer paper) <br/>\n",
    "https://arxiv.org/abs/1409.0473 (2014 first attention mention paper) <br/>\n",
    "https://arxiv.org/abs/1508.04025(2015 attention improved) </br>\n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ (attention blog) <br/>\n",
    "https://jalammar.github.io/illustrated-transformer/ (blog illustrated explanation) <br/>\n",
    "https://nlp.seas.harvard.edu/2018/04/03/attention.html (paper with annotation with code) <br/>\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY (karpathy gpt from scratch video) <br/>\n",
    "https://github.com/karpathy/ng-video-lecture (karpathy gpt from scratch notebooks) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u4PIq2kO0D7t",
   "metadata": {
    "id": "u4PIq2kO0D7t"
   },
   "source": [
    "[google colab](https://colab.research.google.com/drive/1PGCaeDD56NwQfoD4o2qL_zYYtqYdaO3C?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ef2aa23",
   "metadata": {
    "id": "8ef2aa23"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a1d48fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7a1d48fc",
    "outputId": "fa7fcb48-17e1-47a7-ffed-3adf8e72321d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384 # embedding dim\n",
    "n_head = 6 # dim of key, query, value, embedding\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device=='cpu':\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "dropout = 0.2\n",
    "n_layer = 6\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfbd9bee",
   "metadata": {
    "id": "cfbd9bee"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f3aa6e4",
   "metadata": {
    "id": "1f3aa6e4"
   },
   "outputs": [],
   "source": [
    "with open('./data/tinyshakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca4a3b6d",
   "metadata": {
    "id": "ca4a3b6d"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l : \"\".join(itos[c] for c in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da566086",
   "metadata": {
    "id": "da566086"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff0302a1",
   "metadata": {
    "id": "ff0302a1"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    ys = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    xs, ys = xs.to(device), ys.to(device)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "333711c6",
   "metadata": {
    "id": "333711c6"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    o = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        o[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e4f468c",
   "metadata": {
    "id": "0e4f468c"
   },
   "outputs": [],
   "source": [
    "# self attention single head.\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # .register_buffer: to store non-trainable tensors.\n",
    "        # torch.tril will create lower triangle matrix.\n",
    "        # used for masking future chars.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: B,T,C\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        # get the key embedding of each token in the context.\n",
    "        k = self.key(x) # shape: B,T,h\n",
    "\n",
    "        # get the query embedding of each token in the context.\n",
    "        q = self.query(x) # shape: B,T,h\n",
    "\n",
    "        # calc dot product each query embedding with key embedding, this will give\n",
    "        # sort of 'affinitity or attention' score for each token against each other token.\n",
    "        # k.shape[-1]**0.5 : this is kind of initialization, graident stabilzing factor, k.shape[-1] is the\n",
    "        # head size (n_head), this suggested scaling factor as per paper.\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # shape: B,T,h -> B,T,T\n",
    "\n",
    "        # masking of future token. e.g first can only see itself, and second one see itself and prev\n",
    "        # one.\n",
    "        # float('-inf'): because during exponential those will go to zero.\n",
    "        # todo: the need for tril[:T, :T] not clear yet because tril already defined in T shae???\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # shape: B,T,T\n",
    "\n",
    "        # convert wei is probs\n",
    "        wei = F.softmax(wei, dim=-1) # shape: B,T,T\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # get the value embedding of each token.\n",
    "        v = self.value(x) # shape: B,T,h\n",
    "\n",
    "        # multiply the probs score to each token value embedding to get the weighted embedding value.\n",
    "        # todo: review this process reasoning.\n",
    "        # main attention operation: scored sum of all the context value embedding vectors.\n",
    "        out = wei @ v # shape: B,T,T @ B,T,h -> B,T,h\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "115a5e6e",
   "metadata": {
    "id": "115a5e6e"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ba09b8b",
   "metadata": {
    "id": "4ba09b8b"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5e74853",
   "metadata": {
    "id": "a5e74853"
   },
   "outputs": [],
   "source": [
    "# todo: notes.\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "291512fb",
   "metadata": {
    "id": "291512fb"
   },
   "outputs": [],
   "source": [
    "# todo: notes.\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx shape: B,T\n",
    "        for _ in range(max_new_tokens):\n",
    "            # todo: shape change not clear\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # todo: shape is not clear\n",
    "            logits = logits[:, -1, :] # shape: B,C\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # as torch multinomial not working with device 'mps'.\n",
    "            # todo: not always for some cases works??\n",
    "            if 'mps' in str(idx.device):\n",
    "                probs = probs.to('cpu')\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shpae: B,1\n",
    "            if 'mps' in str(idx.device):\n",
    "                idx_next = idx_next.to(device)\n",
    "\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # shape: B,T+1\n",
    "\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16453de2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16453de2",
    "outputId": "67b01cb4-6ca3-4e59-dd4b-3d92f5a4dff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M Parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "opt = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M Parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "082589c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "082589c5",
    "outputId": "34a48a81-f39d-4b39-98b3-bba776992241",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, train loss: 4.2221, val loss: 4.2306\n",
      "epoch:501, train loss: 1.7600, val loss: 1.9146\n",
      "epoch:1001, train loss: 1.3903, val loss: 1.5987\n",
      "epoch:1501, train loss: 1.2644, val loss: 1.5271\n",
      "epoch:2001, train loss: 1.1835, val loss: 1.4978\n",
      "epoch:2501, train loss: 1.1233, val loss: 1.4910\n",
      "epoch:3001, train loss: 1.0718, val loss: 1.4804\n",
      "epoch:3501, train loss: 1.0179, val loss: 1.5127\n",
      "epoch:4001, train loss: 0.9604, val loss: 1.5102\n",
      "epoch:4501, train loss: 0.9125, val loss: 1.5351\n"
     ]
    }
   ],
   "source": [
    "for itr in range(max_iters):\n",
    "    if itr%eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"epoch:{itr+1}, train loss: {losses['train']:.4f}, val loss: {losses['test']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1e27266",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1e27266",
    "outputId": "c01999d0-92c0-4271-d1b4-723ca717eb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But with prison can be as easy to the fimker.\n",
      "Women are you behold, and fled you there,\n",
      "'Tis set upon your trade toward my throats to me\n",
      "brother's blood and by my tale.\n",
      "\n",
      "SLY:\n",
      "This is this, sir; an more of the duty, with\n",
      "inw: comfort, I will venty cannot thing enforce it\n",
      "for a life thing I could to fled till\n",
      "spock in your particular, if reseem sound, 'tis gone,\n",
      "and the presently general perish. Never proud, we have the\n",
      "matter been up in this bed, I cause to be boy very please: the\n",
      "pluck it goes a\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029de92a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "029de92a",
    "outputId": "2ef20a20-fdae-4818-c945-13875cb0167f"
   },
   "outputs": [],
   "source": [
    "# open('./data/after_training_output.txt', 'w').write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5410bb5",
   "metadata": {
    "id": "f5410bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*before training generated text*\n",
      "\n",
      "Ndc.wd,ZTkOLoL,xHtK\n",
      "bHiPWCkMBbzA$3:.aSvgO-3DSMBF?gLTa\n",
      "hXbYVXHthNfNuDqcBMxv.tbVr dXlNDZaLeHFw3$HPDAWk,fDE'nYzxzkkouX\n",
      "Yo3&$LMtofVidIvB!H&V!$W;KdYlNZ,\n",
      "e3 irYeYEvnkci;;lxW;HFtEdroG EsSXHB;qWk J..GD3.FyWjbs!selJljnFAUVQu.$-huD3qoVchvVy:?UuO;MnissX3Uwty.HJ'vBPUHIkBBf&pjY-igvIEjVk:D.yqwJdqNBtSkkqKoaW-SNAFQhdVCeIib3qo'Tjt-&dE$HZLETxg$\n",
      "hx&$FsgC-LKKgAr-xTrH\n",
      "hAxkNxmnvnrufW&A so;;3;QDLWLm:fEE,Cey$vlMUE$tq,fMFMPrD?UqKLS?U.UrHK-NLbk!ar,dyb&i&:\n",
      "ardsabWZ$!VEgDFsYBvuihVKN.k?DUr?AnyHRr-uts-N&fn VEcvNMJkMQEEYFEPk3\n",
      "\n",
      "-----------------\n",
      "\n",
      "*after training generated text*\n",
      "\n",
      "But with prison can be as easy to the fimker.\n",
      "Women are you behold, and fled you there,\n",
      "'Tis set upon your trade toward my throats to me\n",
      "brother's blood and by my tale.\n",
      "\n",
      "SLY:\n",
      "This is this, sir; an more of the duty, with\n",
      "inw: comfort, I will venty cannot thing enforce it\n",
      "for a life thing I could to fled till\n",
      "spock in your particular, if reseem sound, 'tis gone,\n",
      "and the presently general perish. Never proud, we have the\n",
      "matter been up in this bed, I cause to be boy very please: the\n",
      "pluck it goes a\n"
     ]
    }
   ],
   "source": [
    "# output before and after training comparison\n",
    "before_trainig_generated_txt = open('./output/before_training_output.txt', 'r').read()\n",
    "after_trainig_generated_txt = open('./output/after_training_output.txt', 'r').read()\n",
    "print('*before training generated text*')\n",
    "print(before_trainig_generated_txt)\n",
    "print('\\n-----------------\\n')\n",
    "print('*after training generated text*')\n",
    "print(after_trainig_generated_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c4a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
