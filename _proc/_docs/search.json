[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "practice_deep_learning",
    "section": "",
    "text": "This repository contains implementation code for important research papers and starter guides for common deep learning tools.",
    "crumbs": [
      "index.html"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "practice_deep_learning",
    "section": "Install",
    "text": "Install\npip install practice_deep_learning",
    "crumbs": [
      "practice_deep_learning"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "practice_deep_learning",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "practice_deep_learning"
    ]
  },
  {
    "objectID": "index.html#deep-learning-portal",
    "href": "index.html#deep-learning-portal",
    "title": "Deep Learning Portal ðŸ”¥",
    "section": "",
    "text": "This repository contains implementation code for important research papers and starter guides for common deep learning tools.",
    "crumbs": [
      "Deep Learning Portal ðŸ”¥"
    ]
  },
  {
    "objectID": "mini_batch_training.html",
    "href": "mini_batch_training.html",
    "title": "simple 2 layer nn",
    "section": "",
    "text": "data_path = Path('../data/mnist.pkl.gz')\nwith gzip.open(data_path, 'r') as f:\n    ((x_train, y_train), (x_test, y_test), _) = pickle.load(f, encoding='latin') \nx_train, y_train, x_test, y_test = map(torch.tensor, (x_train, y_train, x_test, y_test))\nx_train.shape, y_train.shape, x_test.shape, y_test.shape\n\n(torch.Size([50000, 784]),\n torch.Size([50000]),\n torch.Size([10000, 784]),\n torch.Size([10000]))\n\n\n\nimg = x_train[0]\nimg = img.view(28, 28)\nplt.imshow(img);\nplt.axis('off');\n\n\n\n\n\n\n\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_o):\n        super().__init__()\n        self.layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_o)]\n    \n    def __call__(self, x):\n        for l in self.layers:\n            x = l(x)\n        return x\n\n\nn_in = x_train.shape[1]\nn_h = 50\nn_o = 10\n\nmodel = Model(n_in, n_h, n_o)\npred = model(x_train)\npred.shape\n\ntorch.Size([50000, 10])\n\n\n# cross entropy loss\n\ndef log_softmax(x):\n    return (x.exp()/x.exp().sum(-1, keepdim=True)).log()\n\n\nlog_softmax(pred)\n\ntensor([[-2.3917, -2.3172, -2.1445,  ..., -2.3604, -2.4435, -2.3298],\n        [-2.3426, -2.2119, -2.2799,  ..., -2.3664, -2.4151, -2.2220],\n        [-2.3725, -2.2966, -2.2658,  ..., -2.2858, -2.3270, -2.3698],\n        ...,\n        [-2.4004, -2.3082, -2.1309,  ..., -2.3633, -2.4319, -2.2571],\n        [-2.4322, -2.3229, -2.1224,  ..., -2.3613, -2.4487, -2.2554],\n        [-2.3660, -2.2850, -2.0563,  ..., -2.3602, -2.5124, -2.3140]],\n       grad_fn=&lt;LogBackward0&gt;)\n\n\n# log product to sum trick\n\ndef log_softmax(x):\n    return x - x.exp().sum(-1, keepdim=True).log()\n\n\nlog_softmax(pred)\n\ntensor([[-2.3917, -2.3172, -2.1445,  ..., -2.3604, -2.4435, -2.3298],\n        [-2.3426, -2.2119, -2.2799,  ..., -2.3664, -2.4151, -2.2220],\n        [-2.3725, -2.2966, -2.2658,  ..., -2.2858, -2.3270, -2.3698],\n        ...,\n        [-2.4004, -2.3082, -2.1309,  ..., -2.3633, -2.4319, -2.2571],\n        [-2.4322, -2.3229, -2.1224,  ..., -2.3613, -2.4487, -2.2554],\n        [-2.3660, -2.2850, -2.0563,  ..., -2.3602, -2.5124, -2.3140]],\n       grad_fn=&lt;SubBackward0&gt;)\n\n\n# log sum exp trick\n* normalize with the maximum value, so avoid exploding big activations.\n\ndef logsumexp(x):\n    m = x.max(-1)[-1]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\n\ndef log_softmax(x):\n    return x - logsumexp(x)[:,None]\n\n\nlog_softmax(pred)\n\ntensor([[-2.3917, -2.3172, -2.1445,  ..., -2.3604, -2.4435, -2.3298],\n        [-2.3426, -2.2119, -2.2799,  ..., -2.3664, -2.4151, -2.2220],\n        [-2.3725, -2.2966, -2.2658,  ..., -2.2858, -2.3270, -2.3698],\n        ...,\n        [-2.4004, -2.3082, -2.1309,  ..., -2.3633, -2.4319, -2.2571],\n        [-2.4322, -2.3229, -2.1224,  ..., -2.3613, -2.4487, -2.2554],\n        [-2.3660, -2.2850, -2.0563,  ..., -2.3602, -2.5124, -2.3140]],\n       grad_fn=&lt;SubBackward0&gt;)\n\n\n# pytorch logsumexp function\n\ndef log_softmax(x):\n    return x - x.logsumexp(-1, keepdim=True)\n\n\nlog_softmax(pred)\n\ntensor([[-2.3917, -2.3172, -2.1445,  ..., -2.3604, -2.4435, -2.3298],\n        [-2.3426, -2.2119, -2.2799,  ..., -2.3664, -2.4151, -2.2220],\n        [-2.3725, -2.2966, -2.2658,  ..., -2.2858, -2.3270, -2.3698],\n        ...,\n        [-2.4004, -2.3082, -2.1309,  ..., -2.3633, -2.4319, -2.2571],\n        [-2.4322, -2.3229, -2.1224,  ..., -2.3613, -2.4487, -2.2554],\n        [-2.3660, -2.2850, -2.0563,  ..., -2.3602, -2.5124, -2.3140]],\n       grad_fn=&lt;SubBackward0&gt;)\n\n\n# negative log likeliehood\n* for one hot input vector, it simplifies to the following formula.\n\ndef nll(inp, targ):\n    return - inp[range(targ.shape[0]), targ].mean()\n\n\nsm_pred = log_softmax(pred)\nloss = nll(sm_pred, y_train)\nloss\n\ntensor(2.3028, grad_fn=&lt;NegBackward0&gt;)\n\n\n# compare it with native pytorch implementation of nll.\n\nloss_pytorch = F.nll_loss(F.log_softmax(pred, -1), y_train)\nloss_pytorch\n\ntensor(2.3028, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n# nll and softmax combined implementation.\n\nloss_pytorch = F.cross_entropy(pred, y_train)\nloss_pytorch\n\ntensor(2.3028, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n# batch training.\n# accuracy.\n\ndef accuracy(out, yb):\n    return (out.argmax(1)==yb).float().mean()\n\nloss_func = F.cross_entropy\n\n\nbs = 50\nxb = x_train[:bs]\nyb = y_train[:bs]\npreds = model(xb)\npreds[0]\n\ntensor([-0.0843, -0.0098,  0.1629,  0.1187,  0.1040,  0.0934, -0.1870, -0.0530,\n        -0.1361, -0.0224], grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nloss_func(preds, yb)\n\ntensor(2.2846, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\naccuracy(preds, yb)\n\ntensor(0.1400)\n\n\n\ndef report(loss, preds, yb):\n    print(f\"loss: {loss:.2f}, accuracy: {accuracy(preds, yb):.2f}\")\n\n\nreport(loss, preds, yb)\n\nloss: 2.30, accuracy: 0.14\n\n\n\nn, m = x_train.shape\nlr = 0.5\nepochs = 3\nxb,yb = x_train[:bs], y_train[:bs]\npreds = model(xb)\nloss = loss_func(preds, yb)\nreport(loss, preds, yb)\n\nloss: 2.28, accuracy: 0.14\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(i+bs, n))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias -= l.bias.grad * lr\n                    l.weight.grad.zero_()\n                    l.bias.grad.zero_()\n    report(loss, preds, yb)\n\nloss: 0.17, accuracy: 0.94\nloss: 0.13, accuracy: 0.94\nloss: 0.13, accuracy: 0.96\n\n\n\n# parameters\n\nm1 = nn.Module()\nm1.foo = nn.Linear(3, 4)\nm1.boo = 'hey'\nm1\n\nModule(\n  (foo): Linear(in_features=3, out_features=4, bias=True)\n)\n\n\n\nlist(m1.named_children())\n\n[('foo', Linear(in_features=3, out_features=4, bias=True))]\n\n\n\nlist(m1.parameters())\n\n[Parameter containing:\n tensor([[-0.4626, -0.5572, -0.2930],\n         [-0.2142,  0.2954, -0.5759],\n         [-0.0873,  0.5067,  0.0329],\n         [ 0.1627,  0.2251, -0.2415]], requires_grad=True),\n Parameter containing:\n tensor([-0.4074,  0.0654,  0.3297, -0.2555], requires_grad=True)]\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.l1 = nn.Linear(n_in, n_h)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(n_h, n_out)\n    \n    def forward(self, x):\n        return self.l2(self.relu(self.l1(x)))\n\n\nmodel = MLP(n_in, n_h, 10)\nmodel\n\nMLP(\n  (l1): Linear(in_features=784, out_features=50, bias=True)\n  (relu): ReLU()\n  (l2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nfor name, l in model.named_children():\n    print(f\"{name}: {l}\")\n\nl1: Linear(in_features=784, out_features=50, bias=True)\nrelu: ReLU()\nl2: Linear(in_features=50, out_features=10, bias=True)\n\n\n\nfor p in model.parameters():\n    print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\ndef fit():\n    for epoch in range(epochs):\n        for i in range(0, n, bs):\n            s = slice(i, min(i+bs, n))\n            xb,yb = x_train[s], y_train[s]\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters():\n                    p -= p.grad * lr\n                model.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\n\nloss: 0.02, accuracy: 1.00\nloss: 0.05, accuracy: 0.98\nloss: 0.03, accuracy: 1.00\n\n\n# nn.Module behind the scene\n\nclass MyModule:\n    def __init__(self, n_in, n_h, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in, n_h)\n        self.l2 = nn.Linear(n_h, n_out)\n        self.relu = nn.ReLU()\n    \n    def __setattr__(self, k, v):\n        if not k.startswith('_'):\n            self._modules[k] = v\n        \n        super().__setattr__(k, v)\n    \n    def __repr__(self):\n        return f\"{self._modules}\"\n    \n    def parameters(self):\n        for l in self._modules.values():\n            yield from l.parameters()\n\n\nmdl = MyModule(n_in, n_h, n_o)\nmdl\n\n{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True), 'relu': ReLU()}\n\n\n\nfor p in mdl.parameters():\n    print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n# registering modules\n\nfrom functools import reduce\n\n\nlayers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_o)]\n\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = layers\n        for i,l in enumerate(self.layers):\n            self.add_module(f\"layer_{i}\", l)\n    \n    def forward(self, x):\n        return reduce(lambda val, layer: layer(val), self.layers, x)\n\n\nmodel = Model(layers)\nmodel\n\nModel(\n  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n  (layer_1): ReLU()\n  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nmodel(xb).shape\n\ntorch.Size([50, 10])\n\n\n# nn.ModuleList\n\nclass SequentialModel(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        for l in self.layers:\n            x = l(x)\n        return x\n\n\nmodel = SequentialModel(layers)\nmodel(xb).shape\n\ntorch.Size([50, 10])\n\n\n# nn.Sequential\n\nmodel = nn.Sequential(*layers)\n\n\nfit()\n\nloss: 0.14, accuracy: 0.96\nloss: 0.11, accuracy: 0.96\nloss: 0.05, accuracy: 1.00\n\n\n\nmodel\n\nSequential(\n  (0): Linear(in_features=784, out_features=50, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n# optim\n\nclass Optimizer:\n    def __init__(self, params, lr=0.5):\n        self.params, self.lr = list(params), lr\n    \n    def step(self):\n        with torch.no_grad():\n            for p in self.params:\n                p -= p.grad * self.lr\n\n    def zero_grad(self):\n        for p in self.params:\n            p.grad.data.zero_()\n\n\nmodel = nn.Sequential(nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_o))\n\n\nopt = Optimizer(model.parameters(), lr=lr)\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(i+bs, n))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\nloss: 0.13, accuracy: 0.96\nloss: 0.12, accuracy: 0.92\nloss: 0.08, accuracy: 0.96\n\n\n\nfrom torch import optim\n\n\ndef get_model():\n    model = nn.Sequential(nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_o))\n    opt = optim.SGD(model.parameters(), lr=lr)\n    return opt, model\n\n\nopt, model = get_model()\nloss_func(model(xb), yb)\n\ntensor(2.2912, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        s = slice(i, min(i+bs, n))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\nloss: 0.15, accuracy: 0.96\nloss: 0.11, accuracy: 0.96\nloss: 0.06, accuracy: 1.00\n\n\n# dataset\n\n\nDataset\n\n Dataset (x, y)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntrain_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_test, y_test)\n\n\nopt, model = get_model()\n\n\nfor epoch in range(epochs):\n    for i in range(0, n, bs):\n        xb,yb = train_ds[i: min(i+bs, n)]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n\nloss: 0.13, accuracy: 0.96\nloss: 0.10, accuracy: 0.98\nloss: 0.12, accuracy: 0.96\n\n\n# data loader\n\nclass DataLoader:\n    def __init__(self, ds, bs):\n        self.ds, self.bs = ds, bs\n    \n    def __iter__(self):\n        for i in range(0, len(self.ds), self.bs):\n            yield self.ds[i:i+self.bs]\n\n\ntrain_dl = DataLoader(train_ds, bs)\nvalid_dl = DataLoader(valid_ds, bs)\n\n\nxb, yb = next(iter(train_dl))\nxb.shape\n\ntorch.Size([50, 784])\n\n\n\nopt, model = get_model()\n\n\ndef fit():\n    for epoch in range(epochs):\n        for xb,yb in train_dl:\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        report(loss, preds, yb)\n\n\nfit()\n\nloss: 0.16, accuracy: 0.96\nloss: 0.11, accuracy: 0.98\nloss: 0.07, accuracy: 0.98\n\n\n\n# random sampling\n\nimport random\n\n\nclass Sampler:\n    def __init__(self, ds, shuffle=False):\n        self.n, self.shuffle = len(ds), shuffle\n    \n    def __iter__(self):\n        res = list(range(self.n))\n        if self.shuffle:\n            random.shuffle(res)\n        return iter(res)\n\n\nfrom itertools import islice\n\n\nss = Sampler(train_ds)\n\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nimport fastcore.all as fc\n\n\nclass BatchSampler:\n    def __init__(self, sampler, bs, drop_last=False):\n        fc.store_attr()\n    \n    def __iter__(self):\n        yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n\n\nbatches = BatchSampler(ss, 5)\nlist(islice(iter(batches), 3))\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]\n\n\n\ndef collate(b):\n    xs, ys = zip(*b)\n    return torch.stack(xs), torch.stack(ys)\n\n\nclass DataLoader:\n    def __init__(self, ds, batchs, collate_fn=collate):\n        fc.store_attr()\n    \n    def __iter__(self):\n        yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)\n\n\ntrain_sampler = BatchSampler(Sampler(train_ds, shuffle=True), bs)\nvalid_sampler = BatchSampler(Sampler(valid_ds, shuffle=True), bs)\n\n\ntrain_dl = DataLoader(train_ds, train_sampler)\nvalid_dl = DataLoader(valid_ds, valid_sampler)\n\n\nxb, yb = next(iter(valid_dl))\nxb.shape, yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\nplt.imshow(xb[0].view(28, 28));\nplt.axis('off');\n\n\n\n\n\n\n\n\n\nopt, model = get_model()\n\n\nfit()\n\nloss: 0.11, accuracy: 0.94\nloss: 0.27, accuracy: 0.96\nloss: 0.03, accuracy: 1.00\n\n\n\n# multiprocessing dataloader\n\nimport torch.multiprocessing as mp\n\n\nclass DataLoader:\n    def __init__(self, ds, batchs, collate_fn=collate, num_workers=1):\n        fc.store_attr()\n    \n    def __iter__(self):\n        with mp.Pool(self.num_workers) as ex:\n            yield from ex.map(self.ds.__getitem__, iter(self.batchs))\n\n\ntrain_dl = DataLoader(train_ds, batchs=train_sampler)\n\n\nxb, yb = next(iter(train_dl))\nplt.imshow(xb[0].view(28, 28));\nplt.axis('off');\n\n\n\n\n\n\n\n\n# pytorch dataloaders\n\nt = RandomSampler(train_ds)\n\n\nnext(iter(t))\n\n24797\n\n\n\nt = BatchSampler(train_ds, batch_size=2, drop_last=False)\n\nk  = next(iter(t))\nprint(len(k))\nfor ele in k:\n    print(ele[0].shape, ele[1])\n\n2\ntorch.Size([784]) tensor(5)\ntorch.Size([784]) tensor(0)\n\n\n\nt = BatchSampler(RandomSampler(train_ds), batch_size=2, drop_last=False)\n\nk  = next(iter(t))\nprint(len(k))\nfor ele in k:\n    print(ele)\n\n2\n33683\n36592\n\n\n\ntrain_samp = BatchSampler(RandomSampler(train_ds), bs, drop_last=False)\nvalid_samp = BatchSampler(RandomSampler(valid_ds), bs, drop_last=False)\n\n\ntrain_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\nvalid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)\n\n\nopt, model = get_model()\nfit()\n\nloss: 0.20, accuracy: 0.94\nloss: 0.11, accuracy: 0.98\nloss: 0.20, accuracy: 0.98\n\n\n\ntrain_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=2, drop_last=True)\nvalid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)\n\n\nopt, model = get_model()\nfit()\n\nloss: 0.08, accuracy: 0.98\nloss: 0.31, accuracy: 0.86\nloss: 0.11, accuracy: 0.98\n\n\n\n# validation\n\n\n\nfit\n\n fit (epochs, model, loss_func, opt, train_dl, valid_ld)\n\n\n\n\nget_dls\n\n get_dls (train_ds, valid_ds, bs, **kwargs)\n\n\ntrain_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\nopt, model = get_model()\n\n\n\n\n0 0.1775239165313542 0.948100003004074\n1 0.1179210783354938 0.9646000063419342\n2 0.11550588405691087 0.9665000039339066\n3 0.10593999677803367 0.9698000079393387\n4 0.10098711441038176 0.9727000087499619\nCPU times: user 17.8 s, sys: 16.1 s, total: 33.8 s\nWall time: 4.71 s"
  }
]